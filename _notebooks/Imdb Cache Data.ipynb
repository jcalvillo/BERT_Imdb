{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "from bert_serving.client import BertClient, ConcurrentBertClient\n",
    "from tensorflow.estimator import BaselineClassifier\n",
    "from tensorflow.python.estimator.canned.dnn import DNNClassifier\n",
    "from tensorflow.python.estimator.run_config import RunConfig\n",
    "from tensorflow.python.estimator.training import TrainSpec, EvalSpec, train_and_evaluate\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_parallel_calls = 1\n",
    "bc = BertClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0\n",
      "Chunk 1\n",
      "Chunk 2\n",
      "Chunk 3\n",
      "Chunk 4\n",
      "Chunk 5\n",
      "Chunk 6\n",
      "Chunk 7\n",
      "Wrote data_000.p\n",
      "Chunk 8\n",
      "Chunk 9\n",
      "Chunk 10\n",
      "Chunk 11\n",
      "Chunk 12\n",
      "Chunk 13\n",
      "Chunk 14\n",
      "Chunk 15\n",
      "Wrote data_001.p\n",
      "Chunk 16\n",
      "Chunk 17\n",
      "Chunk 18\n",
      "Chunk 19\n",
      "Chunk 20\n",
      "Chunk 21\n",
      "Chunk 22\n",
      "Chunk 23\n",
      "Wrote data_002.p\n",
      "Chunk 24\n",
      "Chunk 25\n",
      "Chunk 26\n",
      "Chunk 27\n",
      "Chunk 28\n",
      "Chunk 29\n",
      "Chunk 30\n",
      "Chunk 31\n",
      "Wrote data_003.p\n",
      "Chunk 32\n",
      "Chunk 33\n",
      "Chunk 34\n",
      "Chunk 35\n",
      "Chunk 36\n",
      "Chunk 37\n",
      "Chunk 38\n",
      "Chunk 39\n",
      "Wrote data_004.p\n",
      "Chunk 40\n",
      "Chunk 41\n",
      "Chunk 42\n",
      "Chunk 43\n",
      "Chunk 44\n",
      "Chunk 45\n",
      "Chunk 46\n",
      "Chunk 47\n",
      "Wrote data_005.p\n",
      "Chunk 48\n",
      "Chunk 49\n",
      "Chunk 50\n",
      "Chunk 51\n",
      "Chunk 52\n",
      "Chunk 53\n",
      "Chunk 54\n",
      "Chunk 55\n",
      "Wrote data_006.p\n",
      "Chunk 56\n",
      "Chunk 57\n",
      "Chunk 58\n",
      "Chunk 59\n",
      "Chunk 60\n",
      "Chunk 61\n",
      "Chunk 62\n",
      "Chunk 63\n",
      "Wrote data_007.p\n",
      "Chunk 64\n",
      "Chunk 65\n",
      "Chunk 66\n",
      "Chunk 67\n",
      "Chunk 68\n",
      "Chunk 69\n",
      "Chunk 70\n",
      "Chunk 71\n",
      "Wrote data_008.p\n",
      "Chunk 72\n",
      "Chunk 73\n",
      "Chunk 74\n",
      "Chunk 75\n",
      "Chunk 76\n",
      "Chunk 77\n",
      "Chunk 78\n",
      "Chunk 79\n",
      "Wrote data_009.p\n",
      "Chunk 80\n",
      "Chunk 81\n",
      "Chunk 82\n",
      "Chunk 83\n",
      "Chunk 84\n",
      "Chunk 85\n",
      "Chunk 86\n",
      "Chunk 87\n",
      "Wrote data_010.p\n",
      "Chunk 88\n",
      "Chunk 89\n",
      "Chunk 90\n",
      "Chunk 91\n",
      "Chunk 92\n",
      "Chunk 93\n",
      "Chunk 94\n",
      "Chunk 95\n",
      "Wrote data_011.p\n",
      "Chunk 96\n",
      "Chunk 97\n",
      "Wrote data_012.p\n",
      "Chunk 98\n",
      "Chunk 99\n",
      "Chunk 100\n",
      "Chunk 101\n",
      "Chunk 102\n",
      "Chunk 103\n",
      "Chunk 104\n",
      "Chunk 105\n",
      "Wrote data_000.p\n",
      "Chunk 106\n",
      "Chunk 107\n",
      "Chunk 108\n",
      "Chunk 109\n",
      "Chunk 110\n",
      "Chunk 111\n",
      "Chunk 112\n",
      "Chunk 113\n",
      "Wrote data_001.p\n",
      "Chunk 114\n",
      "Chunk 115\n",
      "Chunk 116\n",
      "Chunk 117\n",
      "Chunk 118\n",
      "Chunk 119\n",
      "Chunk 120\n",
      "Chunk 121\n",
      "Wrote data_002.p\n",
      "Chunk 122\n",
      "Chunk 123\n",
      "Chunk 124\n",
      "Chunk 125\n",
      "Chunk 126\n",
      "Chunk 127\n",
      "Chunk 128\n",
      "Chunk 129\n",
      "Wrote data_003.p\n",
      "Chunk 130\n",
      "Chunk 131\n",
      "Chunk 132\n",
      "Chunk 133\n",
      "Chunk 134\n",
      "Chunk 135\n",
      "Chunk 136\n",
      "Chunk 137\n",
      "Wrote data_004.p\n",
      "Chunk 138\n",
      "Chunk 139\n",
      "Chunk 140\n",
      "Chunk 141\n",
      "Chunk 142\n",
      "Chunk 143\n",
      "Chunk 144\n",
      "Chunk 145\n",
      "Wrote data_005.p\n",
      "Chunk 146\n",
      "Chunk 147\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "encode_count = 0\n",
    "def encode(chunk):\n",
    "    global encode_count\n",
    "    print('Chunk {}'.format(encode_count))\n",
    "    encode_count += 1\n",
    "    return bc.encode(chunk)\n",
    "        \n",
    "def get_encodes(data):\n",
    "    # x is `batch_size` of lines, each of which is a json object\n",
    "    features = np.array([])\n",
    "    text = [x[0] for x in data]\n",
    "    features = np.concatenate([encode(chunk) for chunk in chunks(text, 256)])\n",
    "    # randomly choose a label\n",
    "    labels = [x[1] for x in data]\n",
    "    return features, labels\n",
    "\n",
    "def cache_data(data_dir, dest_dir, start_chunk, end_chunk):\n",
    "    pos_files = os.listdir(os.path.join(data_dir, 'pos'))\n",
    "    neg_files = os.listdir(os.path.join(data_dir, 'neg'))\n",
    "        \n",
    "    data = []\n",
    "    for pos_file, neg_file in zip(pos_files, neg_files):\n",
    "        with open(os.path.join(data_dir, 'pos', pos_file)) as f:\n",
    "            review = f.readlines()[0].strip()\n",
    "            data.append((review, 1))\n",
    "        with open(os.path.join(data_dir, 'neg', neg_file)) as f:\n",
    "            review = f.readlines()[0].strip()\n",
    "            data.append((review, 0))\n",
    "    chunk_num = -1\n",
    "    chunk_size = 2048\n",
    "    for chunk in chunks(data, chunk_size):\n",
    "        chunk_num += 1\n",
    "        if chunk_num < start_chunk:\n",
    "            continue\n",
    "        if chunk_num > end_chunk:\n",
    "            break\n",
    "        features, output = get_encodes(chunk)\n",
    "        print('Wrote data_{:03d}.p'.format(chunk_num))\n",
    "        with open(os.path.join(dest_dir, 'data_{:03d}.p'.format(chunk_num)), 'wb') as f:\n",
    "            pickle.dump((features, output), f)\n",
    "\n",
    "input_fn_train = cache_data('/home/eugenet/final_project/data/aclImdb/train/', '/home/eugenet/final_project/cached_data/train_cased_small', 0, 14)\n",
    "input_fn_eval = cache_data('/home/eugenet/final_project/data/aclImdb/test/', '/home/eugenet/final_project/cached_data/test_cased_small', 0, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "?pickle.dump"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
