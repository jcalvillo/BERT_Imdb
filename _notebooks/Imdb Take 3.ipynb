{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to Merge Google's method with my pickled files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_COLUMN = 'sentence'\n",
    "LABEL_COLUMN = 'polarity'\n",
    "# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n",
    "label_list = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "def get_input_fn(data_dir, num_examples=None, num_epochs=10):\n",
    "    data_files = os.listdir(data_dir)\n",
    "    \n",
    "    # open pre-embedded data\n",
    "    feature_list = []\n",
    "    label_list = []\n",
    "    for data_file in data_files:\n",
    "        with open(os.path.join(data_dir, data_file), 'rb') as f:\n",
    "            features, labels = pickle.load(f)\n",
    "            feature_list.append(features)\n",
    "            label_list.append(labels)\n",
    "    features = np.concatenate(feature_list)\n",
    "    labels = [label for labels in label_list for label in labels]\n",
    "    \n",
    "    # split into train and dev set\n",
    "    train_features = features[0:int(0.8*len(features))]\n",
    "    train_labels = labels[0:int(0.8*len(features))]\n",
    "    dev_features = features[int(0.8*len(features)):len(features)]\n",
    "    dev_labels = labels[int(0.8*len(features)):len(features)]\n",
    "    \n",
    "    train_labels = np.array(train_labels).astype('int32')\n",
    "    dev_labels = np.array(dev_labels).astype('int32')\n",
    "    \n",
    "    if num_examples is not None:\n",
    "        train_features = train_features[0:num_examples]\n",
    "        train_labels = train_labels[0:num_examples]\n",
    "    \n",
    "#     print('{} train data points'.format(len(train_features)))\n",
    "#     print('{} dev data points'.format(len(dev_features)))\n",
    "    \n",
    "    train_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={'feature': train_features},\n",
    "        y=train_labels,\n",
    "        num_epochs=num_epochs,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    dev_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={'feature': dev_features},\n",
    "        y=dev_labels,\n",
    "        num_epochs=1,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "    return (train_fn, dev_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = 'train_uncased_small'\n",
    "train_input_fn, dev_input_fn = get_input_fn('/home/eugenet/final_project/cached_data/{}/'.format(bert_model), None, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(is_predicting, features, labels, num_labels):\n",
    "    \"\"\"Creates a classification model.\"\"\"\n",
    "    # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
    "    # Use \"sequence_outputs\" for token-level output.\n",
    "    output_layer = features\n",
    "\n",
    "    hidden_size = output_layer.shape[-1].value\n",
    "\n",
    "    # Create our own layer to tune for politeness data.\n",
    "    output_weights = tf.get_variable(\n",
    "      \"output_weights\", [num_labels, hidden_size],\n",
    "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "    output_bias = tf.get_variable(\n",
    "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "    with tf.variable_scope(\"loss\"):\n",
    "\n",
    "        # Dropout helps prevent overfitting\n",
    "        output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "\n",
    "        logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "        logits = tf.nn.bias_add(logits, output_bias)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "        # Convert labels into one-hot encoding\n",
    "        one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "\n",
    "        predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
    "        # If we're predicting, we want predicted labels and the probabiltiies.\n",
    "        if is_predicting:\n",
    "            return (predicted_labels, log_probs)\n",
    "\n",
    "        # If we're train/eval, compute loss between predicted and actual label\n",
    "        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "        loss = tf.reduce_mean(per_example_loss)\n",
    "        return (loss, predicted_labels, log_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_fn_builder actually creates our model function\n",
    "# using the passed parameters for num_labels, learning_rate, etc.\n",
    "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
    "                     num_warmup_steps):\n",
    "    \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "    def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "        \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "        embeddings = features['feature']\n",
    "        label_ids = labels\n",
    "        print(features, labels)\n",
    "        is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
    "    \n",
    "        # TRAIN and EVAL\n",
    "        if not is_predicting:\n",
    "            (loss, predicted_labels, log_probs) = create_model(\n",
    "                is_predicting, embeddings, label_ids, num_labels)\n",
    "\n",
    "            train_op = bert.optimization.create_optimizer(\n",
    "              loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
    "\n",
    "            # Calculate evaluation metrics. \n",
    "            def metric_fn(label_ids, predicted_labels):\n",
    "                accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
    "                f1_score = tf.contrib.metrics.f1_score(\n",
    "                    label_ids,\n",
    "                    predicted_labels)\n",
    "                auc = tf.metrics.auc(\n",
    "                    label_ids,\n",
    "                    predicted_labels)\n",
    "                recall = tf.metrics.recall(\n",
    "                    label_ids,\n",
    "                    predicted_labels)\n",
    "                precision = tf.metrics.precision(\n",
    "                    label_ids,\n",
    "                    predicted_labels) \n",
    "                true_pos = tf.metrics.true_positives(\n",
    "                    label_ids,\n",
    "                    predicted_labels)\n",
    "                true_neg = tf.metrics.true_negatives(\n",
    "                    label_ids,\n",
    "                    predicted_labels)   \n",
    "                false_pos = tf.metrics.false_positives(\n",
    "                    label_ids,\n",
    "                    predicted_labels)  \n",
    "                false_neg = tf.metrics.false_negatives(\n",
    "                    label_ids,\n",
    "                    predicted_labels)\n",
    "                return {\n",
    "                    \"eval_accuracy\": accuracy,\n",
    "                    \"f1_score\": f1_score,\n",
    "                    \"auc\": auc,\n",
    "                    \"precision\": precision,\n",
    "                    \"recall\": recall,\n",
    "                    \"true_positives\": true_pos,\n",
    "                    \"true_negatives\": true_neg,\n",
    "                    \"false_positives\": false_pos,\n",
    "                    \"false_negatives\": false_neg\n",
    "                }\n",
    "\n",
    "            eval_metrics = metric_fn(label_ids, predicted_labels)\n",
    "\n",
    "            if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "                return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                  loss=loss,\n",
    "                  train_op=train_op)\n",
    "            else:\n",
    "                return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                loss=loss,\n",
    "                eval_metric_ops=eval_metrics)\n",
    "        else:\n",
    "            (predicted_labels, log_probs) = create_model(\n",
    "            is_predicting, embeddings, label_ids, num_labels)\n",
    "\n",
    "            predictions = {\n",
    "              'probabilities': log_probs,\n",
    "              'labels': predicted_labels\n",
    "            }\n",
    "            return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "    # Return the actual model function in the closure\n",
    "    return model_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute train and warmup steps from batch size\n",
    "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 3.0\n",
    "# Warmup is a period of time where hte learning rate \n",
    "# is small and gradually increases--usually helps training.\n",
    "WARMUP_PROPORTION = 0.1\n",
    "# Model configs\n",
    "SAVE_CHECKPOINTS_STEPS = 500\n",
    "SAVE_SUMMARY_STEPS = 100\n",
    "\n",
    "# Compute # train and warmup steps from batch size\n",
    "num_train_steps = int(786 / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '/home/eugenet/final_project/trained_models/imdb_google_example2', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f48f9a24ba8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0409 02:14:59.207397 139952757946176 estimator.py:201] Using config: {'_model_dir': '/home/eugenet/final_project/trained_models/imdb_google_example2', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f48f9a24ba8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "# Specify outpit directory and number of checkpoint steps to save\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    model_dir='/home/eugenet/final_project/trained_models/imdb_google_example2',\n",
    "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)\n",
    "\n",
    "model_fn = model_fn_builder(\n",
    "  num_labels=len(label_list),\n",
    "  learning_rate=LEARNING_RATE,\n",
    "  num_train_steps=num_train_steps,\n",
    "  num_warmup_steps=num_warmup_steps)\n",
    "\n",
    "estimator = tf.estimator.Estimator(\n",
    "  model_fn=model_fn,\n",
    "  config=run_config,\n",
    "  params={\"batch_size\": BATCH_SIZE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Training!\n",
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0409 02:43:30.629841 139952757946176 estimator.py:1111] Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature': <tf.Tensor 'random_shuffle_queue_DequeueUpTo:1' shape=(?, 768) dtype=float32>} Tensor(\"random_shuffle_queue_DequeueUpTo:2\", shape=(?,), dtype=int32, device=/device:CPU:0)\n",
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0409 02:43:31.536596 139952757946176 estimator.py:1113] Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0409 02:43:31.539974 139952757946176 basic_session_run_hooks.py:527] Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0409 02:43:31.632353 139952757946176 monitored_session.py:222] Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/eugenet/final_project/trained_models/imdb_google_example2/model.ckpt-100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0409 02:43:31.639273 139952757946176 saver.py:1270] Restoring parameters from /home/eugenet/final_project/trained_models/imdb_google_example2/model.ckpt-100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/eugenet/miniconda3/envs/bert/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1070: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file utilities to get mtimes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0409 02:43:31.690744 139952757946176 deprecation.py:323] From /home/eugenet/miniconda3/envs/bert/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1070: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file utilities to get mtimes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0409 02:43:31.723484 139952757946176 session_manager.py:491] Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0409 02:43:31.768735 139952757946176 session_manager.py:493] Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 100 into /home/eugenet/final_project/trained_models/imdb_google_example2/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0409 02:43:32.181933 139952757946176 basic_session_run_hooks.py:594] Saving checkpoints for 100 into /home/eugenet/final_project/trained_models/imdb_google_example2/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.64635396, step = 101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0409 02:43:32.563053 139952757946176 basic_session_run_hooks.py:249] loss = 0.64635396, step = 101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 109.788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0409 02:43:33.473622 139952757946176 basic_session_run_hooks.py:680] global_step/sec: 109.788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.66941243, step = 201 (0.912 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0409 02:43:33.475437 139952757946176 basic_session_run_hooks.py:247] loss = 0.66941243, step = 201 (0.912 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 601.267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0409 02:43:33.639870 139952757946176 basic_session_run_hooks.py:680] global_step/sec: 601.267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.6769428, step = 301 (0.166 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0409 02:43:33.641643 139952757946176 basic_session_run_hooks.py:247] loss = 0.6769428, step = 301 (0.166 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 632.094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0409 02:43:33.798088 139952757946176 basic_session_run_hooks.py:680] global_step/sec: 632.094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.6821265, step = 401 (0.159 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0409 02:43:33.800198 139952757946176 basic_session_run_hooks.py:247] loss = 0.6821265, step = 401 (0.159 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 500 into /home/eugenet/final_project/trained_models/imdb_google_example2/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0409 02:43:33.954846 139952757946176 basic_session_run_hooks.py:594] Saving checkpoints for 500 into /home/eugenet/final_project/trained_models/imdb_google_example2/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loss for final step: 0.6505876.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0409 02:43:34.076574 139952757946176 estimator.py:359] Loss for final step: 0.6505876.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took time  0:00:03.508472\n"
     ]
    }
   ],
   "source": [
    "print(f'Beginning Training!')\n",
    "current_time = datetime.now()\n",
    "estimator.train(input_fn=train_input_fn, max_steps=500)\n",
    "print(\"Training took time \", datetime.now() - current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0409 02:43:37.925430 139952757946176 estimator.py:1111] Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature': <tf.Tensor 'fifo_queue_DequeueUpTo:1' shape=(?, 768) dtype=float32>} Tensor(\"fifo_queue_DequeueUpTo:2\", shape=(?,), dtype=int32, device=/device:CPU:0)\n",
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0409 02:43:38.751626 139952757946176 estimator.py:1113] Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2019-04-09T02:43:38Z\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0409 02:43:38.772929 139952757946176 evaluation.py:257] Starting evaluation at 2019-04-09T02:43:38Z\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0409 02:43:38.863159 139952757946176 monitored_session.py:222] Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/eugenet/final_project/trained_models/imdb_google_example2/model.ckpt-500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0409 02:43:38.866685 139952757946176 saver.py:1270] Restoring parameters from /home/eugenet/final_project/trained_models/imdb_google_example2/model.ckpt-500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0409 02:43:38.941139 139952757946176 session_manager.py:491] Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0409 02:43:38.992495 139952757946176 session_manager.py:493] Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2019-04-09-02:43:40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0409 02:43:40.129551 139952757946176 evaluation.py:277] Finished evaluation at 2019-04-09-02:43:40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 500: auc = 0.65139997, eval_accuracy = 0.6514, f1_score = 0.6685681, false_negatives = 742.0, false_positives = 1001.0, global_step = 500, loss = 0.6671107, precision = 0.63718736, recall = 0.7032, true_negatives = 1499.0, true_positives = 1758.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0409 02:43:40.131109 139952757946176 estimator.py:1979] Saving dict for global step 500: auc = 0.65139997, eval_accuracy = 0.6514, f1_score = 0.6685681, false_negatives = 742.0, false_positives = 1001.0, global_step = 500, loss = 0.6671107, precision = 0.63718736, recall = 0.7032, true_negatives = 1499.0, true_positives = 1758.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 500: /home/eugenet/final_project/trained_models/imdb_google_example2/model.ckpt-500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0409 02:43:40.132520 139952757946176 estimator.py:2039] Saving 'checkpoint_path' summary for global step 500: /home/eugenet/final_project/trained_models/imdb_google_example2/model.ckpt-500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'auc': 0.65139997,\n",
       " 'eval_accuracy': 0.6514,\n",
       " 'f1_score': 0.6685681,\n",
       " 'false_negatives': 742.0,\n",
       " 'false_positives': 1001.0,\n",
       " 'loss': 0.6671107,\n",
       " 'precision': 0.63718736,\n",
       " 'recall': 0.7032,\n",
       " 'true_negatives': 1499.0,\n",
       " 'true_positives': 1758.0,\n",
       " 'global_step': 500}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.evaluate(input_fn=dev_input_fn, steps=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
