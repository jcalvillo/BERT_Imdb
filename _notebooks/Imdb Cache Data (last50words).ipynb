{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "from bert_serving.client import BertClient, ConcurrentBertClient\n",
    "from tensorflow.estimator import BaselineClassifier\n",
    "from tensorflow.python.estimator.canned.dnn import DNNClassifier\n",
    "from tensorflow.python.estimator.run_config import RunConfig\n",
    "from tensorflow.python.estimator.training import TrainSpec, EvalSpec, train_and_evaluate\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_parallel_calls = 1\n",
    "bc = BertClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_count = 0\n",
    "def encode(chunk):\n",
    "    global encode_count\n",
    "    print('Chunk {}'.format(encode_count))\n",
    "    encode_count += 1\n",
    "    return bc.encode(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encodes(data):\n",
    "    # x is `batch_size` of lines, each of which is a json object\n",
    "    features = np.array([])\n",
    "    text = [x[0] for x in data]\n",
    "    text = text[len(text)-50:]                                                         # take the last 50 words\n",
    "    features = np.concatenate([encode(chunk) for chunk in chunks(text, 256)])\n",
    "    # randomly choose a label\n",
    "    labels = [x[1] for x in data]\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_data(data_dir, dest_dir, start_chunk, end_chunk):\n",
    "    pos_files = os.listdir(os.path.join(data_dir, 'pos'))\n",
    "    neg_files = os.listdir(os.path.join(data_dir, 'neg'))\n",
    "        \n",
    "    data = []\n",
    "    for pos_file, neg_file in zip(pos_files, neg_files):\n",
    "        with open(os.path.join(data_dir, 'pos', pos_file)) as f:\n",
    "            review = f.readlines()[0].strip()\n",
    "            data.append((review, 1))\n",
    "        with open(os.path.join(data_dir, 'neg', neg_file)) as f:\n",
    "            review = f.readlines()[0].strip()\n",
    "            data.append((review, 0))\n",
    "    chunk_num = -1\n",
    "    chunk_size = 2048\n",
    "    for chunk in chunks(data, chunk_size):\n",
    "        chunk_num += 1\n",
    "        if chunk_num < start_chunk:\n",
    "            continue\n",
    "        if chunk_num > end_chunk:\n",
    "            break\n",
    "        features, output = get_encodes(chunk)\n",
    "        print('Wrote data_{:03d}.p'.format(chunk_num))\n",
    "        with open(os.path.join(dest_dir, 'data_{:03d}.p'.format(chunk_num)), 'wb') as f:\n",
    "            pickle.dump((features, output), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !bert-serving-start -model_dir ./uncased_L-12_H-768_A-12 -num_worker=4 -max_seq_len=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 55\n",
      "Wrote data_000.p\n",
      "Chunk 56\n",
      "Wrote data_001.p\n",
      "Chunk 57\n",
      "Wrote data_002.p\n",
      "Chunk 58\n",
      "Wrote data_003.p\n",
      "Chunk 59\n",
      "Wrote data_004.p\n",
      "Chunk 60\n",
      "Wrote data_005.p\n",
      "Chunk 61\n",
      "Wrote data_006.p\n",
      "Chunk 62\n",
      "Wrote data_007.p\n",
      "Chunk 63\n",
      "Wrote data_008.p\n",
      "Chunk 64\n",
      "Wrote data_009.p\n",
      "Chunk 65\n",
      "Wrote data_010.p\n",
      "Chunk 66\n",
      "Wrote data_011.p\n",
      "Chunk 67\n",
      "Wrote data_012.p\n",
      "Chunk 68\n",
      "Wrote data_000.p\n",
      "Chunk 69\n",
      "Wrote data_001.p\n",
      "Chunk 70\n",
      "Wrote data_002.p\n",
      "Chunk 71\n",
      "Wrote data_003.p\n",
      "Chunk 72\n",
      "Wrote data_004.p\n",
      "Chunk 73\n",
      "Wrote data_005.p\n",
      "Chunk 74\n",
      "Wrote data_006.p\n",
      "Chunk 75\n",
      "Wrote data_007.p\n",
      "Chunk 76\n",
      "Wrote data_008.p\n",
      "Chunk 77\n",
      "Wrote data_009.p\n",
      "Chunk 78\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_fn_train = cache_data('/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/aclImdb/train', '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/uncased_L-12_H-768_A-12/cache/train_last50words_tokens50', 0, 14)\n",
    "input_fn_eval = cache_data('/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/aclImdb/test', '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/uncased_L-12_H-768_A-12/cache/test_last50words_tokens50', 0, 14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "416px",
    "left": "11px",
    "right": "20px",
    "top": "504px",
    "width": "346px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
