{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "from bert_serving.client import BertClient, ConcurrentBertClient\n",
    "from tensorflow.estimator import BaselineClassifier\n",
    "from tensorflow.python.estimator.canned.dnn import DNNClassifier\n",
    "from tensorflow.python.estimator.run_config import RunConfig\n",
    "from tensorflow.python.estimator.training import TrainSpec, EvalSpec, train_and_evaluate\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_parallel_calls = 1\n",
    "bc = BertClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = 200\n",
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "encode_count = 0\n",
    "def encode(chunk):\n",
    "    global encode_count\n",
    "    print('Chunk {}'.format(encode_count))\n",
    "    encode_count += 1\n",
    "    return bc.encode(chunk)\n",
    "        \n",
    "def get_encodes(data):\n",
    "    # x is `batch_size` of lines, each of which is a json object\n",
    "    features = np.array([])\n",
    "    \n",
    "    # TODO: modified\n",
    "    text = []\n",
    "    for x in data:\n",
    "#         tokens = x[0].split(' ')\n",
    "#         if len(tokens) > max_seq_len:\n",
    "#             beginning = tokens[0:int(max_seq_len/2)]\n",
    "#             end = tokens[int(-max_seq_len/2):]\n",
    "#             new_review = '{} ... {}'.format(' '.join(beginning), ' '.join(end))\n",
    "#             text.append(new_review.lower())\n",
    "#         else:\n",
    "        text.append(x[0].lower())\n",
    "    features = np.concatenate([encode(chunk) for chunk in chunks(text, 256)])\n",
    "    # randomly choose a label\n",
    "    labels = [x[1] for x in data]\n",
    "    return features, labels\n",
    "\n",
    "def cache_data(data_dir, dest_dir, start_chunk, end_chunk):\n",
    "    pos_files = os.listdir(os.path.join(data_dir, 'pos'))\n",
    "    neg_files = os.listdir(os.path.join(data_dir, 'neg'))\n",
    "        \n",
    "    data = []\n",
    "    for pos_file, neg_file in zip(pos_files, neg_files):\n",
    "        with open(os.path.join(data_dir, 'pos', pos_file)) as f:\n",
    "            review = f.readlines()[0].strip()\n",
    "            data.append((review, 1))\n",
    "        with open(os.path.join(data_dir, 'neg', neg_file)) as f:\n",
    "            review = f.readlines()[0].strip()\n",
    "            data.append((review, 0))\n",
    "    chunk_num = -1\n",
    "    chunk_size = 2048\n",
    "    for chunk in chunks(data, chunk_size):\n",
    "        chunk_num += 1\n",
    "        if chunk_num < start_chunk:\n",
    "            continue\n",
    "        if chunk_num > end_chunk:\n",
    "            break\n",
    "        features, output = get_encodes(chunk)\n",
    "        print('Wrote data_{:03d}.p'.format(chunk_num))\n",
    "        with open(os.path.join(dest_dir, 'data_{:03d}.p'.format(chunk_num)), 'wb') as f:\n",
    "            pickle.dump((features, output), f)\n",
    "\n",
    "# input_fn_train = cache_data('/home/eugenet/final_project/data/aclImdb/train/', '/home/eugenet/final_project/cached_data/train_uncased_large_max200_frontback', 0, 14)\n",
    "input_fn_eval = cache_data('/home/eugenet/final_project/data/aclImdb/test/', '/home/eugenet/final_project/cached_data/test_uncased_large_max200', 0, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0408 01:14:23.880115 140294087993152 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "# This is a path to an uncased (all lowercase) version of BERT\n",
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "        with tf.Session() as sess:\n",
    "            vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "      \n",
    "    return bert.tokenization.FullTokenizer(\n",
    "        vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20648670196533203"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "cosine(bc.encode(['subwordinfo']), bc.encode(['infodorwsub']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
