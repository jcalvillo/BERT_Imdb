{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ensure jupyter notebook and bert-serving-start are running inside the virtualenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---   \n",
    "  \n",
    "# BERT experiments with Imdb dataset  \n",
    "\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from bert_serving.client import BertClient, ConcurrentBertClient\n",
    "from tensorflow.estimator import BaselineClassifier\n",
    "from tensorflow.python.estimator.canned.dnn import DNNClassifier\n",
    "from tensorflow.python.estimator.run_config import RunConfig\n",
    "from tensorflow.python.estimator.training import TrainSpec, EvalSpec, train_and_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "batch_size = 128\n",
    "num_parallel_calls = 1\n",
    "bc = BertClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "---   \n",
    "  \n",
    "## Cache data\n",
    "\n",
    "---  \n",
    "\n",
    "In order to avoid having to encode embeddings for the dataset every experiment, we create separate files for BERT encodings. Creating this 'cache' is performed in chunks for practical considerations (in case of errors). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# pipeline\n",
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "encode_count = 0\n",
    "def encode(chunk):\n",
    "    global encode_count\n",
    "    print('Chunk {}'.format(encode_count))\n",
    "    encode_count += 1\n",
    "    return bc.encode(chunk)\n",
    "        \n",
    "def get_encodes(data):\n",
    "    # x is `batch_size` of lines, each of which is a json object\n",
    "    features = np.array([])\n",
    "    text = [x[0] for x in data]\n",
    "    features = np.concatenate([encode(chunk) for chunk in chunks(text, 256)])\n",
    "    # randomly choose a label\n",
    "    labels = [x[1] for x in data]\n",
    "    return features, labels\n",
    "\n",
    "def cache_data(data_dir, dest_dir, start_chunk, end_chunk):\n",
    "    pos_files = os.listdir(os.path.join(data_dir, 'pos'))\n",
    "    neg_files = os.listdir(os.path.join(data_dir, 'neg'))\n",
    "        \n",
    "    data = []\n",
    "    for pos_file, neg_file in zip(pos_files, neg_files):\n",
    "        with open(os.path.join(data_dir, 'pos', pos_file)) as f:\n",
    "            review = f.readlines()[0].strip()\n",
    "            data.append((review, 1))\n",
    "        with open(os.path.join(data_dir, 'neg', neg_file)) as f:\n",
    "            review = f.readlines()[0].strip()\n",
    "            data.append((review, 0))\n",
    "    chunk_num = -1\n",
    "    chunk_size = 2048\n",
    "    for chunk in chunks(data, chunk_size):\n",
    "        chunk_num += 1\n",
    "        if chunk_num < start_chunk:\n",
    "            continue\n",
    "        if chunk_num > end_chunk:\n",
    "            break\n",
    "        features, output = get_encodes(chunk)\n",
    "        print('Wrote data_{:03d}.p'.format(chunk_num))\n",
    "        with open(os.path.join(dest_dir, 'data_{:03d}.p'.format(chunk_num)), 'wb') as f:\n",
    "            pickle.dump((features, output), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### \tBERT-Base, Uncased\n",
    "12-layer, 768-hidden, 12-heads, 110M parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# # !bert-serving-start -model_dir ./uncased_L-12_H-768_A-12 -num_worker=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/bert_serving/client/__init__.py:286: UserWarning: some of your sentences have more tokens than \"max_seq_len=25\" set on the server, as consequence you may get less-accurate or truncated embeddings.\n",
      "here is what you can do:\n",
      "- disable the length-check by create a new \"BertClient(check_length=False)\" when you do not want to display this warning\n",
      "- or, start a new server with a larger \"max_seq_len\"\n",
      "  '- or, start a new server with a larger \"max_seq_len\"' % self.length_limit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1\n",
      "Chunk 2\n",
      "Chunk 3\n",
      "Chunk 4\n",
      "Chunk 5\n",
      "Chunk 6\n",
      "Chunk 7\n",
      "Wrote data_000.p\n",
      "Chunk 8\n",
      "Chunk 9\n",
      "Chunk 10\n",
      "Chunk 11\n",
      "Chunk 12\n",
      "Chunk 13\n",
      "Chunk 14\n",
      "Chunk 15\n",
      "Wrote data_001.p\n",
      "Chunk 16\n",
      "Chunk 17\n",
      "Chunk 18\n",
      "Chunk 19\n",
      "Chunk 20\n",
      "Chunk 21\n",
      "Chunk 22\n",
      "Chunk 23\n",
      "Wrote data_002.p\n",
      "Chunk 24\n",
      "Chunk 25\n",
      "Chunk 26\n",
      "Chunk 27\n",
      "Chunk 28\n",
      "Chunk 29\n",
      "Chunk 30\n",
      "Chunk 31\n",
      "Wrote data_003.p\n",
      "Chunk 32\n",
      "Chunk 33\n",
      "Chunk 34\n",
      "Chunk 35\n",
      "Chunk 36\n",
      "Chunk 37\n",
      "Chunk 38\n",
      "Chunk 39\n",
      "Wrote data_004.p\n",
      "Chunk 40\n",
      "Chunk 41\n",
      "Chunk 42\n",
      "Chunk 43\n",
      "Chunk 44\n",
      "Chunk 45\n",
      "Chunk 46\n",
      "Chunk 47\n",
      "Wrote data_005.p\n",
      "Chunk 48\n",
      "Chunk 49\n",
      "Chunk 50\n",
      "Chunk 51\n",
      "Chunk 52\n",
      "Chunk 53\n",
      "Chunk 54\n",
      "Chunk 55\n",
      "Wrote data_006.p\n",
      "Chunk 56\n",
      "Chunk 57\n",
      "Chunk 58\n",
      "Chunk 59\n",
      "Chunk 60\n",
      "Chunk 61\n",
      "Chunk 62\n",
      "Chunk 63\n",
      "Wrote data_007.p\n",
      "Chunk 64\n",
      "Chunk 65\n",
      "Chunk 66\n",
      "Chunk 67\n",
      "Chunk 68\n",
      "Chunk 69\n",
      "Chunk 70\n",
      "Chunk 71\n",
      "Wrote data_008.p\n",
      "Chunk 72\n",
      "Chunk 73\n",
      "Chunk 74\n",
      "Chunk 75\n",
      "Chunk 76\n",
      "Chunk 77\n",
      "Chunk 78\n",
      "Chunk 79\n",
      "Wrote data_009.p\n",
      "Chunk 80\n",
      "Chunk 81\n",
      "Chunk 82\n",
      "Chunk 83\n",
      "Chunk 84\n",
      "Chunk 85\n",
      "Chunk 86\n",
      "Chunk 87\n",
      "Wrote data_010.p\n",
      "Chunk 88\n",
      "Chunk 89\n",
      "Chunk 90\n",
      "Chunk 91\n",
      "Chunk 92\n",
      "Chunk 93\n",
      "Chunk 94\n",
      "Chunk 95\n",
      "Wrote data_011.p\n",
      "Chunk 96\n",
      "Chunk 97\n",
      "Wrote data_012.p\n",
      "Chunk 98\n",
      "Chunk 99\n",
      "Chunk 100\n",
      "Chunk 101\n",
      "Chunk 102\n",
      "Chunk 103\n",
      "Chunk 104\n",
      "Chunk 105\n",
      "Wrote data_000.p\n",
      "Chunk 106\n",
      "Chunk 107\n",
      "Chunk 108\n",
      "Chunk 109\n",
      "Chunk 110\n",
      "Chunk 111\n",
      "Chunk 112\n",
      "Chunk 113\n",
      "Wrote data_001.p\n",
      "Chunk 114\n",
      "Chunk 115\n",
      "Chunk 116\n",
      "Chunk 117\n",
      "Chunk 118\n",
      "Chunk 119\n",
      "Chunk 120\n",
      "Chunk 121\n",
      "Wrote data_002.p\n",
      "Chunk 122\n",
      "Chunk 123\n",
      "Chunk 124\n",
      "Chunk 125\n",
      "Chunk 126\n",
      "Chunk 127\n",
      "Chunk 128\n",
      "Chunk 129\n",
      "Wrote data_003.p\n",
      "Chunk 130\n",
      "Chunk 131\n",
      "Chunk 132\n",
      "Chunk 133\n",
      "Chunk 134\n",
      "Chunk 135\n",
      "Chunk 136\n",
      "Chunk 137\n",
      "Wrote data_004.p\n",
      "Chunk 138\n",
      "Chunk 139\n",
      "Chunk 140\n",
      "Chunk 141\n",
      "Chunk 142\n",
      "Chunk 143\n",
      "Chunk 144\n",
      "Chunk 145\n",
      "Wrote data_005.p\n",
      "Chunk 146\n",
      "Chunk 147\n",
      "Chunk 148\n",
      "Chunk 149\n",
      "Chunk 150\n",
      "Chunk 151\n",
      "Chunk 152\n",
      "Chunk 153\n",
      "Wrote data_006.p\n",
      "Chunk 154\n",
      "Chunk 155\n",
      "Chunk 156\n",
      "Chunk 157\n",
      "Chunk 158\n",
      "Chunk 159\n",
      "Chunk 160\n",
      "Chunk 161\n",
      "Wrote data_007.p\n",
      "Chunk 162\n",
      "Chunk 163\n",
      "Chunk 164\n",
      "Chunk 165\n",
      "Chunk 166\n",
      "Chunk 167\n",
      "Chunk 168\n",
      "Chunk 169\n",
      "Wrote data_008.p\n",
      "Chunk 170\n",
      "Chunk 171\n",
      "Chunk 172\n",
      "Chunk 173\n",
      "Chunk 174\n",
      "Chunk 175\n",
      "Chunk 176\n",
      "Chunk 177\n",
      "Wrote data_009.p\n",
      "Chunk 178\n",
      "Chunk 179\n",
      "Chunk 180\n",
      "Chunk 181\n",
      "Chunk 182\n",
      "Chunk 183\n",
      "Chunk 184\n",
      "Chunk 185\n",
      "Wrote data_010.p\n",
      "Chunk 186\n",
      "Chunk 187\n",
      "Chunk 188\n",
      "Chunk 189\n",
      "Chunk 190\n",
      "Chunk 191\n",
      "Chunk 192\n",
      "Chunk 193\n",
      "Wrote data_011.p\n",
      "Chunk 194\n",
      "Chunk 195\n",
      "Wrote data_012.p\n",
      "CPU times: user 3.09 s, sys: 1.73 s, total: 4.83 s\n",
      "Wall time: 35min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# set directories\n",
    "my_dir_train = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/BERT_Imdb/aclImdb/train'\n",
    "my_dir_test  = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/aclImdb/test'\n",
    "my_dir_train_output = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/uncased_L-12_H-768_A-12/cache/train'\n",
    "my_dir_test_output  = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/uncased_L-12_H-768_A-12/cache/test'\n",
    "\n",
    "# build cache\n",
    "input_fn_train = cache_data(my_dir_train, my_dir_train_output, 0, 14)\n",
    "input_fn_eval = cache_data(my_dir_test, my_dir_test_output, 0, 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \tBERT-Base, Cased\n",
    "12-layer, 768-hidden, 12-heads , 110M parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/bert_serving/client/__init__.py:286: UserWarning: some of your sentences have more tokens than \"max_seq_len=50\" set on the server, as consequence you may get less-accurate or truncated embeddings.\n",
      "here is what you can do:\n",
      "- disable the length-check by create a new \"BertClient(check_length=False)\" when you do not want to display this warning\n",
      "- or, start a new server with a larger \"max_seq_len\"\n",
      "  '- or, start a new server with a larger \"max_seq_len\"' % self.length_limit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote data_000.p\n",
      "Wrote data_001.p\n",
      "Wrote data_002.p\n",
      "Wrote data_003.p\n",
      "Wrote data_004.p\n",
      "Wrote data_005.p\n",
      "Wrote data_006.p\n",
      "Wrote data_007.p\n",
      "Wrote data_008.p\n",
      "Wrote data_009.p\n",
      "Wrote data_010.p\n",
      "Wrote data_011.p\n",
      "Wrote data_012.p\n",
      "Wrote data_000.p\n",
      "Wrote data_001.p\n",
      "Wrote data_002.p\n",
      "Wrote data_003.p\n",
      "Wrote data_004.p\n",
      "Wrote data_005.p\n",
      "Wrote data_006.p\n",
      "Wrote data_007.p\n",
      "Wrote data_008.p\n",
      "Wrote data_009.p\n",
      "Wrote data_010.p\n",
      "Wrote data_011.p\n",
      "Wrote data_012.p\n",
      "CPU times: user 3.87 s, sys: 4.66 s, total: 8.53 s\n",
      "Wall time: 29min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# set directories\n",
    "my_dir_train = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/aclImdb/train'\n",
    "my_dir_test  = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/aclImdb/test'\n",
    "my_dir_train_output = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/cased_L-12_H-768_A-12/cache/train'\n",
    "my_dir_test_output  = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/cased_L-12_H-768_A-12/cache/test'\n",
    "\n",
    "# build cache\n",
    "input_fn_train = cache_data(my_dir_train, my_dir_train_output, 0, 14)\n",
    "input_fn_eval = cache_data(my_dir_test, my_dir_test_output, 0, 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \tBERT-Large, Uncased\n",
    "24-layer, 1024-hidden, 16-heads, 340M parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/bert_serving/client/__init__.py:286: UserWarning: some of your sentences have more tokens than \"max_seq_len=50\" set on the server, as consequence you may get less-accurate or truncated embeddings.\n",
      "here is what you can do:\n",
      "- disable the length-check by create a new \"BertClient(check_length=False)\" when you do not want to display this warning\n",
      "- or, start a new server with a larger \"max_seq_len\"\n",
      "  '- or, start a new server with a larger \"max_seq_len\"' % self.length_limit)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# set directories\n",
    "my_dir_train = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/aclImdb/train'\n",
    "my_dir_test  = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/aclImdb/test'\n",
    "my_dir_train_output = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/uncased_L-24_H-1024_A-16/cache/train'\n",
    "my_dir_test_output  = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/uncased_L-24_H-1024_A-16/cache/test'\n",
    "\n",
    "# build cache\n",
    "input_fn_train = cache_data(my_dir_train, my_dir_train_output, 0, 14)\n",
    "input_fn_eval = cache_data(my_dir_test, my_dir_test_output, 0, 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT-Large, Cased\n",
    "24-layer, 1024-hidden, 16-heads, 340M parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/bert_serving/client/__init__.py:286: UserWarning: some of your sentences have more tokens than \"max_seq_len=25\" set on the server, as consequence you may get less-accurate or truncated embeddings.\n",
      "here is what you can do:\n",
      "- disable the length-check by create a new \"BertClient(check_length=False)\" when you do not want to display this warning\n",
      "- or, start a new server with a larger \"max_seq_len\"\n",
      "  '- or, start a new server with a larger \"max_seq_len\"' % self.length_limit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 589\n",
      "Chunk 590\n",
      "Chunk 591\n",
      "Chunk 592\n",
      "Chunk 593\n",
      "Chunk 594\n",
      "Chunk 595\n",
      "Wrote data_000.p\n",
      "Chunk 596\n",
      "Chunk 597\n",
      "Chunk 598\n",
      "Chunk 599\n",
      "Chunk 600\n",
      "Chunk 601\n",
      "Chunk 602\n",
      "Chunk 603\n",
      "Wrote data_001.p\n",
      "Chunk 604\n",
      "Chunk 605\n",
      "Chunk 606\n",
      "Chunk 607\n",
      "Chunk 608\n",
      "Chunk 609\n",
      "Chunk 610\n",
      "Chunk 611\n",
      "Wrote data_002.p\n",
      "Chunk 612\n",
      "Chunk 613\n",
      "Chunk 614\n",
      "Chunk 615\n",
      "Chunk 616\n",
      "Chunk 617\n",
      "Chunk 618\n",
      "Chunk 619\n",
      "Wrote data_003.p\n",
      "Chunk 620\n",
      "Chunk 621\n",
      "Chunk 622\n",
      "Chunk 623\n",
      "Chunk 624\n",
      "Chunk 625\n",
      "Chunk 626\n",
      "Chunk 627\n",
      "Wrote data_004.p\n",
      "Chunk 628\n",
      "Chunk 629\n",
      "Chunk 630\n",
      "Chunk 631\n",
      "Chunk 632\n",
      "Chunk 633\n",
      "Chunk 634\n",
      "Chunk 635\n",
      "Wrote data_005.p\n",
      "Chunk 636\n",
      "Chunk 637\n",
      "Chunk 638\n",
      "Chunk 639\n",
      "Chunk 640\n",
      "Chunk 641\n",
      "Chunk 642\n",
      "Chunk 643\n",
      "Wrote data_006.p\n",
      "Chunk 644\n",
      "Chunk 645\n",
      "Chunk 646\n",
      "Chunk 647\n",
      "Chunk 648\n",
      "Chunk 649\n",
      "Chunk 650\n",
      "Chunk 651\n",
      "Wrote data_007.p\n",
      "Chunk 652\n",
      "Chunk 653\n",
      "Chunk 654\n",
      "Chunk 655\n",
      "Chunk 656\n",
      "Chunk 657\n",
      "Chunk 658\n",
      "Chunk 659\n",
      "Wrote data_008.p\n",
      "Chunk 660\n",
      "Chunk 661\n",
      "Chunk 662\n",
      "Chunk 663\n",
      "Chunk 664\n",
      "Chunk 665\n",
      "Chunk 666\n",
      "Chunk 667\n",
      "Wrote data_009.p\n",
      "Chunk 668\n",
      "Chunk 669\n",
      "Chunk 670\n",
      "Chunk 671\n",
      "Chunk 672\n",
      "Chunk 673\n",
      "Chunk 674\n",
      "Chunk 675\n",
      "Wrote data_010.p\n",
      "Chunk 676\n",
      "Chunk 677\n",
      "Chunk 678\n",
      "Chunk 679\n",
      "Chunk 680\n",
      "Chunk 681\n",
      "Chunk 682\n",
      "Chunk 683\n",
      "Wrote data_011.p\n",
      "Chunk 684\n",
      "Chunk 685\n",
      "Wrote data_012.p\n",
      "Chunk 686\n",
      "Chunk 687\n",
      "Chunk 688\n",
      "Chunk 689\n",
      "Chunk 690\n",
      "Chunk 691\n",
      "Chunk 692\n",
      "Chunk 693\n",
      "Wrote data_000.p\n",
      "Chunk 694\n",
      "Chunk 695\n",
      "Chunk 696\n",
      "Chunk 697\n",
      "Chunk 698\n",
      "Chunk 699\n",
      "Chunk 700\n",
      "Chunk 701\n",
      "Wrote data_001.p\n",
      "Chunk 702\n",
      "Chunk 703\n",
      "Chunk 704\n",
      "Chunk 705\n",
      "Chunk 706\n",
      "Chunk 707\n",
      "Chunk 708\n",
      "Chunk 709\n",
      "Wrote data_002.p\n",
      "Chunk 710\n",
      "Chunk 711\n",
      "Chunk 712\n",
      "Chunk 713\n",
      "Chunk 714\n",
      "Chunk 715\n",
      "Chunk 716\n",
      "Chunk 717\n",
      "Wrote data_003.p\n",
      "Chunk 718\n",
      "Chunk 719\n",
      "Chunk 720\n",
      "Chunk 721\n",
      "Chunk 722\n",
      "Chunk 723\n",
      "Chunk 724\n",
      "Chunk 725\n",
      "Wrote data_004.p\n",
      "Chunk 726\n",
      "Chunk 727\n",
      "Chunk 728\n",
      "Chunk 729\n",
      "Chunk 730\n",
      "Chunk 731\n",
      "Chunk 732\n",
      "Chunk 733\n",
      "Wrote data_005.p\n",
      "Chunk 734\n",
      "Chunk 735\n",
      "Chunk 736\n",
      "Chunk 737\n",
      "Chunk 738\n",
      "Chunk 739\n",
      "Chunk 740\n",
      "Chunk 741\n",
      "Wrote data_006.p\n",
      "Chunk 742\n",
      "Chunk 743\n",
      "Chunk 744\n",
      "Chunk 745\n",
      "Chunk 746\n",
      "Chunk 747\n",
      "Chunk 748\n",
      "Chunk 749\n",
      "Wrote data_007.p\n",
      "Chunk 750\n",
      "Chunk 751\n",
      "Chunk 752\n",
      "Chunk 753\n",
      "Chunk 754\n",
      "Chunk 755\n",
      "Chunk 756\n",
      "Chunk 757\n",
      "Wrote data_008.p\n",
      "Chunk 758\n",
      "Chunk 759\n",
      "Chunk 760\n",
      "Chunk 761\n",
      "Chunk 762\n",
      "Chunk 763\n",
      "Chunk 764\n",
      "Chunk 765\n",
      "Wrote data_009.p\n",
      "Chunk 766\n",
      "Chunk 767\n",
      "Chunk 768\n",
      "Chunk 769\n",
      "Chunk 770\n",
      "Chunk 771\n",
      "Chunk 772\n",
      "Chunk 773\n",
      "Wrote data_010.p\n",
      "Chunk 774\n",
      "Chunk 775\n",
      "Chunk 776\n",
      "Chunk 777\n",
      "Chunk 778\n",
      "Chunk 779\n",
      "Chunk 780\n",
      "Chunk 781\n",
      "Wrote data_011.p\n",
      "Chunk 782\n",
      "Chunk 783\n",
      "Wrote data_012.p\n",
      "CPU times: user 3.84 s, sys: 3.62 s, total: 7.46 s\n",
      "Wall time: 34min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# set directories\n",
    "my_dir_train = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/aclImdb/train'\n",
    "my_dir_test  = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/aclImdb/test'\n",
    "my_dir_train_output = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/cased_L-24_H-1024_A-16/cache/train'\n",
    "my_dir_test_output  = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/cased_L-24_H-1024_A-16/cache/test'\n",
    "\n",
    "# build cache\n",
    "input_fn_train = cache_data(my_dir_train, my_dir_train_output, 0, 14)\n",
    "input_fn_eval = cache_data(my_dir_test, my_dir_test_output, 0, 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preparation for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT-small, uncased\n",
    "dir_BSU_train = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/uncased_L-12_H-768_A-12/cache/train'\n",
    "dir_BSU_test  = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/uncased_L-12_H-768_A-12/cache/test'\n",
    "# BERT-large, uncased\n",
    "dir_BLU_train = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/uncased_L-24_H-1024_A-16/cache/train'\n",
    "dir_BLU_test  = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/uncased_L-24_H-1024_A-16/cache/test'\n",
    "# BERT-small, cased\n",
    "dir_BSC_train = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/cased_L-12_H-768_A-12/cache/train'\n",
    "dir_BSC_test  = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/cased_L-12_H-768_A-12/cache/test'\n",
    "# BERT-large, cased\n",
    "dir_BLC_train = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/cased_L-24_H-1024_A-16/cache/train'\n",
    "dir_BLC_test  = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/cased_L-24_H-1024_A-16/cache/test'\n",
    "\n",
    "# Model output directory\n",
    "bert_model, classifier, input_size, train_max_steps, hidden_units, learning_rate, dropout_rate = '','','','','','',''\n",
    "dir_models = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_{}_{}_input{}_maxsteps{}_hu{}_lr{}_dropout{}'.format(\n",
    "    bert_model, classifier, input_size, train_max_steps, '_'.join([str(x) for x in hidden_units]), learning_rate, dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encodes(data):\n",
    "    # x is `batch_size` of lines, each of which is a json object\n",
    "    features = bc.encode([x[0] for x in data])\n",
    "    # randomly choose a label\n",
    "    labels = [x[1] for x in data]\n",
    "    return features, labels\n",
    "\n",
    "def get_input_fn(data_dir, num_examples=None, num_epochs=10):\n",
    "    data_files = os.listdir(data_dir)\n",
    "    \n",
    "    # open pre-embedded data\n",
    "    feature_list = []\n",
    "    label_list = []\n",
    "    for data_file in data_files:\n",
    "        with open(os.path.join(data_dir, data_file), 'rb') as f:\n",
    "            features, labels = pickle.load(f)\n",
    "            feature_list.append(features)\n",
    "            label_list.append(labels)\n",
    "    features = np.concatenate(feature_list)\n",
    "    labels = [label for labels in label_list for label in labels]\n",
    "    \n",
    "    # split into train and dev set\n",
    "    train_features = features[0:int(0.8*len(features))]\n",
    "    train_labels = labels[0:int(0.8*len(features))]\n",
    "    dev_features = features[int(0.8*len(features)):len(features)]\n",
    "    dev_labels = labels[int(0.8*len(features)):len(features)]\n",
    "    \n",
    "    train_labels = np.array(train_labels).astype('int32')\n",
    "    dev_labels = np.array(dev_labels).astype('int32')\n",
    "    \n",
    "    if num_examples is not None:\n",
    "        train_features = train_features[0:num_examples]\n",
    "        train_labels = train_labels[0:num_examples]\n",
    "    \n",
    "    print('{} train data points'.format(len(train_features)))\n",
    "    print('{} dev data points'.format(len(dev_features)))\n",
    "    \n",
    "    train_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={'feature': train_features},\n",
    "        y=train_labels,\n",
    "        num_epochs=num_epochs,\n",
    "        batch_size=128,\n",
    "        shuffle=True\n",
    "    )\n",
    "    dev_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={'feature': dev_features},\n",
    "        y=dev_labels,\n",
    "        num_epochs=1,\n",
    "        batch_size=128,\n",
    "        shuffle=False\n",
    "    )\n",
    "    return (train_fn, dev_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "---   \n",
    "  \n",
    "## Expiriment: Learning Rate\n",
    "\n",
    "---  \n",
    "When the learning rate is too large, gradient descent can inadvertently increase rather than decrease the training error. When the learning rate is too small, training is not only slower, but may become permanently stuck with a high training error.\n",
    "\n",
    "It is not possible to calculate the best learning rate a priori. One approach to tuning this hyperparameter is to run a grid search (e.g., run with learning rates in a range like [.1, .01, .001, .0001 , .0001], and plot to see how key performance indicators change). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### @ 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# !bert-serving-start -model_dir ./uncased_L-12_H-768_A-12 -num_worker=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "saving to:\n",
      "\n",
      " /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.1_dropout0.2 \n",
      "\n",
      "\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.1_dropout0.2', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 100, '_save_checkpoints_secs': None, '_session_config': , '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x13487acc0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "20000 train data points\n",
      "5000 dev data points\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.1_dropout0.2/model.ckpt-1563\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1563 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.1_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:loss = 75.034935, step = 1564\n",
      "INFO:tensorflow:Saving checkpoints for 1663 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.1_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 349.993\n",
      "INFO:tensorflow:loss = 76.748825, step = 1664 (0.287 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1763 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.1_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 505.91\n",
      "INFO:tensorflow:loss = 69.065765, step = 1764 (0.197 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1863 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.1_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 524.78\n",
      "INFO:tensorflow:loss = 69.1278, step = 1864 (0.190 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1963 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.1_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 524.076\n",
      "INFO:tensorflow:loss = 73.80167, step = 1964 (0.191 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.1_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 72.50536.\n",
      "CPU times: user 2.55 s, sys: 326 ms, total: 2.87 s\n",
      "Wall time: 2.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "hidden_units = [10]\n",
    "learning_rate = 0.1\n",
    "bert_model = 'bert_uncased_small'\n",
    "classifier = 'DNN'\n",
    "dropout_rate = 0.2\n",
    "train_max_steps = 2000\n",
    "input_size = None # all\n",
    "\n",
    "dir_models = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_{}_{}_input{}_maxsteps{}_hu{}_lr{}_dropout{}'.format(\n",
    "    bert_model, classifier, input_size, train_max_steps, '_'.join([str(x) for x in hidden_units]), learning_rate, dropout_rate)\n",
    "print ('\\n\\nsaving to:\\n\\n',dir_models, '\\n\\n')\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "run_config = RunConfig(model_dir = dir_models, session_config=config,save_checkpoints_steps=100)\n",
    "estimator = DNNClassifier(\n",
    "    hidden_units=hidden_units,\n",
    "    feature_columns=[tf.feature_column.numeric_column('feature', shape=(768,))],\n",
    "    n_classes=2,\n",
    "    config=run_config,\n",
    "    optimizer=tf.train.AdagradOptimizer(learning_rate=learning_rate),\n",
    "    dropout=dropout_rate)\n",
    "train_input_fn, dev_input_fn = get_input_fn(dir_BSU_train, input_size) # BERT-small, uncased\n",
    "estimator.train(input_fn=train_input_fn, max_steps=train_max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-07-06:30:54\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.1_dropout0.2/model.ckpt-2000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-07-06:30:55\n",
      "INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.7138, accuracy_baseline = 0.5, auc = 0.7891901, auc_precision_recall = 0.7955693, average_loss = 0.5596144, global_step = 2000, label/mean = 0.5, loss = 69.9518, precision = 0.74507105, prediction/mean = 0.51458234, recall = 0.65\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2000: /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.1_dropout0.2/model.ckpt-2000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LR_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.713800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_baseline</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc</th>\n",
       "      <td>0.789190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc_precision_recall</th>\n",
       "      <td>0.795569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>average_loss</th>\n",
       "      <td>0.559614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label/mean</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loss</th>\n",
       "      <td>69.951797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.745071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction/mean</th>\n",
       "      <td>0.514582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>global_step</th>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             LR_1\n",
       "accuracy                 0.713800\n",
       "accuracy_baseline        0.500000\n",
       "auc                      0.789190\n",
       "auc_precision_recall     0.795569\n",
       "average_loss             0.559614\n",
       "label/mean               0.500000\n",
       "loss                    69.951797\n",
       "precision                0.745071\n",
       "prediction/mean          0.514582\n",
       "recall                   0.650000\n",
       "global_step           2000.000000"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result\n",
    "df_LR1 = pd.DataFrame.from_dict(estimator.evaluate(dev_input_fn), orient='index', columns=['LR_1'])\n",
    "df_LR1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### @ 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "saving to:\n",
      "\n",
      " /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.05_dropout0.2 \n",
      "\n",
      "\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.05_dropout0.2', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 100, '_save_checkpoints_secs': None, '_session_config': , '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x133b4c860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "20000 train data points\n",
      "5000 dev data points\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.05_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:loss = 108.71768, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 100 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.05_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 400.993\n",
      "INFO:tensorflow:loss = 76.55812, step = 101 (0.251 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 200 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.05_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 573.684\n",
      "INFO:tensorflow:loss = 79.440575, step = 201 (0.174 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 300 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.05_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 588.748\n",
      "INFO:tensorflow:loss = 71.455635, step = 301 (0.170 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 400 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.05_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 598.924\n",
      "INFO:tensorflow:loss = 75.866806, step = 401 (0.167 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 500 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.05_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 566.251\n",
      "INFO:tensorflow:loss = 73.91676, step = 501 (0.176 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 600 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.05_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 594.332\n",
      "INFO:tensorflow:loss = 68.86103, step = 601 (0.169 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 700 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.05_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 586.259\n",
      "INFO:tensorflow:loss = 64.799545, step = 701 (0.170 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 800 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.05_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 597.518\n",
      "INFO:tensorflow:loss = 76.94132, step = 801 (0.167 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 900 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.05_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 568.86\n",
      "INFO:tensorflow:loss = 66.90614, step = 901 (0.176 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.05_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 589.116\n",
      "INFO:tensorflow:loss = 77.11702, step = 1001 (0.170 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1100 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.05_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 599.753\n",
      "INFO:tensorflow:loss = 72.82083, step = 1101 (0.167 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1200 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.05_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 586.755\n",
      "INFO:tensorflow:loss = 76.32954, step = 1201 (0.170 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1300 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.05_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 584.669\n",
      "INFO:tensorflow:loss = 75.52585, step = 1301 (0.171 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1400 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.05_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 594.738\n",
      "INFO:tensorflow:loss = 73.16197, step = 1401 (0.168 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1500 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.05_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 596.716\n",
      "INFO:tensorflow:loss = 76.52292, step = 1501 (0.168 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1563 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.05_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 35.347656.\n",
      "CPU times: user 5.47 s, sys: 684 ms, total: 6.15 s\n",
      "Wall time: 3.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "hidden_units = [10]\n",
    "learning_rate = 0.05\n",
    "bert_model = 'bert_uncased_small'\n",
    "classifier = 'DNN'\n",
    "dropout_rate = 0.2\n",
    "train_max_steps = 2000\n",
    "input_size = None # all\n",
    "\n",
    "dir_models = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_{}_{}_input{}_maxsteps{}_hu{}_lr{}_dropout{}'.format(\n",
    "    bert_model, classifier, input_size, train_max_steps, '_'.join([str(x) for x in hidden_units]), learning_rate, dropout_rate)\n",
    "print ('\\n\\nsaving to:\\n\\n',dir_models, '\\n\\n')\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "run_config = RunConfig(model_dir=dir_models, session_config=config,save_checkpoints_steps=100)\n",
    "estimator = DNNClassifier(\n",
    "    hidden_units=hidden_units,\n",
    "    feature_columns=[tf.feature_column.numeric_column('feature', shape=(768,))],\n",
    "    n_classes=2,\n",
    "    config=run_config,\n",
    "    optimizer=tf.train.AdagradOptimizer(learning_rate=learning_rate),\n",
    "    dropout=dropout_rate)\n",
    "train_input_fn, dev_input_fn = get_input_fn(dir_BSU_train, input_size) # BERT-small, uncased\n",
    "estimator.train(input_fn=train_input_fn, max_steps=train_max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-07-06:30:59\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.05_dropout0.2/model.ckpt-1563\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-07-06:31:00\n",
      "INFO:tensorflow:Saving dict for global step 1563: accuracy = 0.7038, accuracy_baseline = 0.5, auc = 0.7782934, auc_precision_recall = 0.7903569, average_loss = 0.5676637, global_step = 1563, label/mean = 0.5, loss = 70.95796, precision = 0.7448342, prediction/mean = 0.5072773, recall = 0.62\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1563: /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.05_dropout0.2/model.ckpt-1563\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LR_05</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.703800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_baseline</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc</th>\n",
       "      <td>0.778293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc_precision_recall</th>\n",
       "      <td>0.790357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>average_loss</th>\n",
       "      <td>0.567664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label/mean</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loss</th>\n",
       "      <td>70.957962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.744834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction/mean</th>\n",
       "      <td>0.507277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>global_step</th>\n",
       "      <td>1563.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            LR_05\n",
       "accuracy                 0.703800\n",
       "accuracy_baseline        0.500000\n",
       "auc                      0.778293\n",
       "auc_precision_recall     0.790357\n",
       "average_loss             0.567664\n",
       "label/mean               0.500000\n",
       "loss                    70.957962\n",
       "precision                0.744834\n",
       "prediction/mean          0.507277\n",
       "recall                   0.620000\n",
       "global_step           1563.000000"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result\n",
    "df_LR05 = pd.DataFrame.from_dict(estimator.evaluate(dev_input_fn), orient='index', columns=['LR_05'])\n",
    "df_LR05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### @ 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "saving to:\n",
      "\n",
      " /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.01_dropout0.2 \n",
      "\n",
      "\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.01_dropout0.2', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 100, '_save_checkpoints_secs': None, '_session_config': , '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1339f8828>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "20000 train data points\n",
      "5000 dev data points\n",
      "INFO:tensorflow:Skipping training since max_steps has already saved.\n",
      "CPU times: user 56 ms, sys: 50.3 ms, total: 106 ms\n",
      "Wall time: 103 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "hidden_units = [10]\n",
    "learning_rate = 0.01\n",
    "bert_model = 'bert_uncased_small'\n",
    "classifier = 'DNN'\n",
    "dropout_rate = 0.2\n",
    "train_max_steps = 2000\n",
    "input_size = None # all\n",
    "\n",
    "dir_models = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_{}_{}_input{}_maxsteps{}_hu{}_lr{}_dropout{}'.format(\n",
    "    bert_model, classifier, input_size, train_max_steps, '_'.join([str(x) for x in hidden_units]), learning_rate, dropout_rate)\n",
    "print ('\\n\\nsaving to:\\n\\n',dir_models, '\\n\\n')\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "run_config = RunConfig(model_dir=dir_models, session_config=config,save_checkpoints_steps=100)\n",
    "estimator = DNNClassifier(\n",
    "    hidden_units=hidden_units,\n",
    "    feature_columns=[tf.feature_column.numeric_column('feature', shape=(768,))],\n",
    "    n_classes=2,\n",
    "    config=run_config,\n",
    "    optimizer=tf.train.AdagradOptimizer(learning_rate=learning_rate),\n",
    "    dropout=dropout_rate)\n",
    "train_input_fn, dev_input_fn = get_input_fn(dir_BSU_train, input_size) # BERT-small, uncased\n",
    "estimator.train(input_fn=train_input_fn, max_steps=train_max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-07-06:31:01\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.01_dropout0.2/model.ckpt-2000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-07-06:31:02\n",
      "INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.7126, accuracy_baseline = 0.5, auc = 0.79420185, auc_precision_recall = 0.7977605, average_loss = 0.544821, global_step = 2000, label/mean = 0.5, loss = 68.10263, precision = 0.7276231, prediction/mean = 0.5082678, recall = 0.6796\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2000: /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.01_dropout0.2/model.ckpt-2000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LR_01</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.712600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_baseline</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc</th>\n",
       "      <td>0.794202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc_precision_recall</th>\n",
       "      <td>0.797760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>average_loss</th>\n",
       "      <td>0.544821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label/mean</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loss</th>\n",
       "      <td>68.102631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.727623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction/mean</th>\n",
       "      <td>0.508268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.679600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>global_step</th>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            LR_01\n",
       "accuracy                 0.712600\n",
       "accuracy_baseline        0.500000\n",
       "auc                      0.794202\n",
       "auc_precision_recall     0.797760\n",
       "average_loss             0.544821\n",
       "label/mean               0.500000\n",
       "loss                    68.102631\n",
       "precision                0.727623\n",
       "prediction/mean          0.508268\n",
       "recall                   0.679600\n",
       "global_step           2000.000000"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result\n",
    "df_LR01 = pd.DataFrame.from_dict(estimator.evaluate(dev_input_fn), orient='index', columns=['LR_01'])\n",
    "df_LR01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### @ 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "saving to:\n",
      "\n",
      " /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.001_dropout0.2 \n",
      "\n",
      "\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.001_dropout0.2', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 100, '_save_checkpoints_secs': None, '_session_config': , '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x132d850f0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "20000 train data points\n",
      "5000 dev data points\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.001_dropout0.2/model.ckpt-1563\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1563 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.001_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:loss = 75.22359, step = 1564\n",
      "INFO:tensorflow:Saving checkpoints for 1663 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.001_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 387.961\n",
      "INFO:tensorflow:loss = 92.73311, step = 1664 (0.259 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1763 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.001_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 561.41\n",
      "INFO:tensorflow:loss = 73.14006, step = 1764 (0.178 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1863 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.001_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 547.235\n",
      "INFO:tensorflow:loss = 77.72366, step = 1864 (0.183 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1963 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.001_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 580.514\n",
      "INFO:tensorflow:loss = 76.23874, step = 1964 (0.173 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.001_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 77.54425.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x132d85390>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_units = [10]\n",
    "learning_rate = 0.001\n",
    "bert_model = 'bert_uncased_small'\n",
    "classifier = 'DNN'\n",
    "dropout_rate = 0.2\n",
    "train_max_steps = 2000\n",
    "input_size = None # all\n",
    "\n",
    "dir_models = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_{}_{}_input{}_maxsteps{}_hu{}_lr{}_dropout{}'.format(\n",
    "    bert_model, classifier, input_size, train_max_steps, '_'.join([str(x) for x in hidden_units]), learning_rate, dropout_rate)\n",
    "print ('\\n\\nsaving to:\\n\\n',dir_models, '\\n\\n')\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "run_config = RunConfig(model_dir=dir_models, session_config=config,save_checkpoints_steps=100)\n",
    "estimator = DNNClassifier(\n",
    "    hidden_units=hidden_units,\n",
    "    feature_columns=[tf.feature_column.numeric_column('feature', shape=(768,))],\n",
    "    n_classes=2,\n",
    "    config=run_config,\n",
    "    optimizer=tf.train.AdagradOptimizer(learning_rate=learning_rate),\n",
    "    dropout=dropout_rate)\n",
    "train_input_fn, dev_input_fn = get_input_fn(dir_BSU_train, input_size) # BERT-small, uncased\n",
    "estimator.train(input_fn=train_input_fn, max_steps=train_max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-07-06:31:05\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.001_dropout0.2/model.ckpt-2000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-07-06:31:05\n",
      "INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.6904, accuracy_baseline = 0.5, auc = 0.7625588, auc_precision_recall = 0.7616379, average_loss = 0.59062976, global_step = 2000, label/mean = 0.5, loss = 73.82872, precision = 0.69224554, prediction/mean = 0.49986613, recall = 0.6856\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2000: /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.001_dropout0.2/model.ckpt-2000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LR_001</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.690400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_baseline</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc</th>\n",
       "      <td>0.762559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc_precision_recall</th>\n",
       "      <td>0.761638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>average_loss</th>\n",
       "      <td>0.590630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label/mean</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loss</th>\n",
       "      <td>73.828720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.692246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction/mean</th>\n",
       "      <td>0.499866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.685600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>global_step</th>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           LR_001\n",
       "accuracy                 0.690400\n",
       "accuracy_baseline        0.500000\n",
       "auc                      0.762559\n",
       "auc_precision_recall     0.761638\n",
       "average_loss             0.590630\n",
       "label/mean               0.500000\n",
       "loss                    73.828720\n",
       "precision                0.692246\n",
       "prediction/mean          0.499866\n",
       "recall                   0.685600\n",
       "global_step           2000.000000"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result\n",
    "df_LR001 = pd.DataFrame.from_dict(estimator.evaluate(dev_input_fn), orient='index', columns=['LR_001'])\n",
    "df_LR001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### @ 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "saving to:\n",
      "\n",
      " /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.0001_dropout0.2 \n",
      "\n",
      "\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.0001_dropout0.2', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 100, '_save_checkpoints_secs': None, '_session_config': , '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x134562978>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "20000 train data points\n",
      "5000 dev data points\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.0001_dropout0.2/model.ckpt-1563\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1563 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.0001_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:loss = 87.4829, step = 1564\n",
      "INFO:tensorflow:Saving checkpoints for 1663 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.0001_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 402.15\n",
      "INFO:tensorflow:loss = 89.72653, step = 1664 (0.250 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1763 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.0001_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 583.131\n",
      "INFO:tensorflow:loss = 89.00325, step = 1764 (0.172 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1863 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.0001_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 595.441\n",
      "INFO:tensorflow:loss = 88.17384, step = 1864 (0.168 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1963 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.0001_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 590.42\n",
      "INFO:tensorflow:loss = 88.33943, step = 1964 (0.170 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.0001_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 91.62637.\n",
      "CPU times: user 2.23 s, sys: 264 ms, total: 2.5 s\n",
      "Wall time: 1.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "hidden_units = [10]\n",
    "learning_rate = 0.0001\n",
    "bert_model = 'bert_uncased_small'\n",
    "classifier = 'DNN'\n",
    "dropout_rate = 0.2\n",
    "train_max_steps = 2000\n",
    "input_size = None # all\n",
    "\n",
    "dir_models = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_{}_{}_input{}_maxsteps{}_hu{}_lr{}_dropout{}'.format(\n",
    "    bert_model, classifier, input_size, train_max_steps, '_'.join([str(x) for x in hidden_units]), learning_rate, dropout_rate)\n",
    "print ('\\n\\nsaving to:\\n\\n',dir_models, '\\n\\n')\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "run_config = RunConfig(model_dir=dir_models, session_config=config,save_checkpoints_steps=100)\n",
    "estimator = DNNClassifier(\n",
    "    hidden_units=hidden_units,\n",
    "    feature_columns=[tf.feature_column.numeric_column('feature', shape=(768,))],\n",
    "    n_classes=2,\n",
    "    config=run_config,\n",
    "    optimizer=tf.train.AdagradOptimizer(learning_rate=learning_rate),\n",
    "    dropout=dropout_rate)\n",
    "train_input_fn, dev_input_fn = get_input_fn(dir_BSU_train, input_size) # BERT-small, uncased\n",
    "estimator.train(input_fn=train_input_fn, max_steps=train_max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-07-06:31:08\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.0001_dropout0.2/model.ckpt-2000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-07-06:31:09\n",
      "INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.5888, accuracy_baseline = 0.5, auc = 0.6162612, auc_precision_recall = 0.602811, average_loss = 0.6750571, global_step = 2000, label/mean = 0.5, loss = 84.38214, precision = 0.58090377, prediction/mean = 0.5056232, recall = 0.6376\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2000: /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr0.0001_dropout0.2/model.ckpt-2000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LR_0001</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.588800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_baseline</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc</th>\n",
       "      <td>0.616261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc_precision_recall</th>\n",
       "      <td>0.602811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>average_loss</th>\n",
       "      <td>0.675057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label/mean</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loss</th>\n",
       "      <td>84.382141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.580904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction/mean</th>\n",
       "      <td>0.505623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.637600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>global_step</th>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          LR_0001\n",
       "accuracy                 0.588800\n",
       "accuracy_baseline        0.500000\n",
       "auc                      0.616261\n",
       "auc_precision_recall     0.602811\n",
       "average_loss             0.675057\n",
       "label/mean               0.500000\n",
       "loss                    84.382141\n",
       "precision                0.580904\n",
       "prediction/mean          0.505623\n",
       "recall                   0.637600\n",
       "global_step           2000.000000"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result\n",
    "df_LR0001 = pd.DataFrame.from_dict(estimator.evaluate(dev_input_fn), orient='index', columns=['LR_0001'])\n",
    "df_LR0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### @ 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "saving to:\n",
      "\n",
      " /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr1e-05_dropout0.2 \n",
      "\n",
      "\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr1e-05_dropout0.2', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 100, '_save_checkpoints_secs': None, '_session_config': , '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1339b00b8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "20000 train data points\n",
      "5000 dev data points\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr1e-05_dropout0.2/model.ckpt-1563\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1563 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr1e-05_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:loss = 94.95196, step = 1564\n",
      "INFO:tensorflow:Saving checkpoints for 1663 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr1e-05_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 389.417\n",
      "INFO:tensorflow:loss = 90.27536, step = 1664 (0.258 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1763 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr1e-05_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 544.457\n",
      "INFO:tensorflow:loss = 92.99318, step = 1764 (0.184 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1863 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr1e-05_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 563.761\n",
      "INFO:tensorflow:loss = 94.547104, step = 1864 (0.177 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1963 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr1e-05_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 541.56\n",
      "INFO:tensorflow:loss = 94.749374, step = 1964 (0.185 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr1e-05_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 95.17322.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-07-06:31:11\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr1e-05_dropout0.2/model.ckpt-2000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-07-06:31:12\n",
      "INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.5112, accuracy_baseline = 0.5, auc = 0.5534019, auc_precision_recall = 0.5399217, average_loss = 0.71737605, global_step = 2000, label/mean = 0.5, loss = 89.672005, precision = 0.50577796, prediction/mean = 0.6174993, recall = 0.9804\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2000: /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr1e-05_dropout0.2/model.ckpt-2000\n",
      "CPU times: user 3.83 s, sys: 336 ms, total: 4.17 s\n",
      "Wall time: 3.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "hidden_units = [10]\n",
    "learning_rate = 0.00001\n",
    "bert_model = 'bert_uncased_small'\n",
    "classifier = 'DNN'\n",
    "dropout_rate = 0.2\n",
    "train_max_steps = 2000\n",
    "input_size = None # all\n",
    "\n",
    "dir_models = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_{}_{}_input{}_maxsteps{}_hu{}_lr{}_dropout{}'.format(\n",
    "    bert_model, classifier, input_size, train_max_steps, '_'.join([str(x) for x in hidden_units]), learning_rate, dropout_rate)\n",
    "print ('\\n\\nsaving to:\\n\\n',dir_models, '\\n\\n')\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "run_config = RunConfig(model_dir=dir_models, session_config=config,save_checkpoints_steps=100)\n",
    "estimator = DNNClassifier(\n",
    "    hidden_units=hidden_units,\n",
    "    feature_columns=[tf.feature_column.numeric_column('feature', shape=(768,))],\n",
    "    n_classes=2,\n",
    "    config=run_config,\n",
    "    optimizer=tf.train.AdagradOptimizer(learning_rate=learning_rate),\n",
    "    dropout=dropout_rate)\n",
    "train_input_fn, dev_input_fn = get_input_fn(dir_BSU_train, input_size) # BERT-small, uncased\n",
    "estimator.train(input_fn=train_input_fn, max_steps=train_max_steps)\n",
    "estimator.evaluate(dev_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-07-06:31:13\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr1e-05_dropout0.2/model.ckpt-2000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-07-06:31:13\n",
      "INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.5112, accuracy_baseline = 0.5, auc = 0.5534019, auc_precision_recall = 0.5399217, average_loss = 0.71737605, global_step = 2000, label/mean = 0.5, loss = 89.672005, precision = 0.50577796, prediction/mean = 0.6174993, recall = 0.9804\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2000: /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_DNN_inputNone_maxsteps2000_hu10_lr1e-05_dropout0.2/model.ckpt-2000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LR_00001</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.511200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_baseline</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc</th>\n",
       "      <td>0.553402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc_precision_recall</th>\n",
       "      <td>0.539922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>average_loss</th>\n",
       "      <td>0.717376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label/mean</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loss</th>\n",
       "      <td>89.672005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.505778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction/mean</th>\n",
       "      <td>0.617499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.980400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>global_step</th>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         LR_00001\n",
       "accuracy                 0.511200\n",
       "accuracy_baseline        0.500000\n",
       "auc                      0.553402\n",
       "auc_precision_recall     0.539922\n",
       "average_loss             0.717376\n",
       "label/mean               0.500000\n",
       "loss                    89.672005\n",
       "precision                0.505778\n",
       "prediction/mean          0.617499\n",
       "recall                   0.980400\n",
       "global_step           2000.000000"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result\n",
    "df_LR00001 = pd.DataFrame.from_dict(estimator.evaluate(dev_input_fn), orient='index', columns=['LR_00001'])\n",
    "df_LR00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LR_1</th>\n",
       "      <th>LR_05</th>\n",
       "      <th>LR_01</th>\n",
       "      <th>LR_001</th>\n",
       "      <th>LR_0001</th>\n",
       "      <th>LR_00001</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.713800</td>\n",
       "      <td>0.703800</td>\n",
       "      <td>0.712600</td>\n",
       "      <td>0.690400</td>\n",
       "      <td>0.588800</td>\n",
       "      <td>0.511200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_baseline</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc</th>\n",
       "      <td>0.789190</td>\n",
       "      <td>0.778293</td>\n",
       "      <td>0.794202</td>\n",
       "      <td>0.762559</td>\n",
       "      <td>0.616261</td>\n",
       "      <td>0.553402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc_precision_recall</th>\n",
       "      <td>0.795569</td>\n",
       "      <td>0.790357</td>\n",
       "      <td>0.797760</td>\n",
       "      <td>0.761638</td>\n",
       "      <td>0.602811</td>\n",
       "      <td>0.539922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>average_loss</th>\n",
       "      <td>0.559614</td>\n",
       "      <td>0.567664</td>\n",
       "      <td>0.544821</td>\n",
       "      <td>0.590630</td>\n",
       "      <td>0.675057</td>\n",
       "      <td>0.717376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label/mean</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loss</th>\n",
       "      <td>69.951797</td>\n",
       "      <td>70.957962</td>\n",
       "      <td>68.102631</td>\n",
       "      <td>73.828720</td>\n",
       "      <td>84.382141</td>\n",
       "      <td>89.672005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.745071</td>\n",
       "      <td>0.744834</td>\n",
       "      <td>0.727623</td>\n",
       "      <td>0.692246</td>\n",
       "      <td>0.580904</td>\n",
       "      <td>0.505778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction/mean</th>\n",
       "      <td>0.514582</td>\n",
       "      <td>0.507277</td>\n",
       "      <td>0.508268</td>\n",
       "      <td>0.499866</td>\n",
       "      <td>0.505623</td>\n",
       "      <td>0.617499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>0.679600</td>\n",
       "      <td>0.685600</td>\n",
       "      <td>0.637600</td>\n",
       "      <td>0.980400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>global_step</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>1563.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             LR_1        LR_05        LR_01       LR_001  \\\n",
       "accuracy                 0.713800     0.703800     0.712600     0.690400   \n",
       "accuracy_baseline        0.500000     0.500000     0.500000     0.500000   \n",
       "auc                      0.789190     0.778293     0.794202     0.762559   \n",
       "auc_precision_recall     0.795569     0.790357     0.797760     0.761638   \n",
       "average_loss             0.559614     0.567664     0.544821     0.590630   \n",
       "label/mean               0.500000     0.500000     0.500000     0.500000   \n",
       "loss                    69.951797    70.957962    68.102631    73.828720   \n",
       "precision                0.745071     0.744834     0.727623     0.692246   \n",
       "prediction/mean          0.514582     0.507277     0.508268     0.499866   \n",
       "recall                   0.650000     0.620000     0.679600     0.685600   \n",
       "global_step           2000.000000  1563.000000  2000.000000  2000.000000   \n",
       "\n",
       "                          LR_0001     LR_00001  \n",
       "accuracy                 0.588800     0.511200  \n",
       "accuracy_baseline        0.500000     0.500000  \n",
       "auc                      0.616261     0.553402  \n",
       "auc_precision_recall     0.602811     0.539922  \n",
       "average_loss             0.675057     0.717376  \n",
       "label/mean               0.500000     0.500000  \n",
       "loss                    84.382141    89.672005  \n",
       "precision                0.580904     0.505778  \n",
       "prediction/mean          0.505623     0.617499  \n",
       "recall                   0.637600     0.980400  \n",
       "global_step           2000.000000  2000.000000  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([df_LR1, df_LR05, df_LR01, df_LR001, df_LR0001, df_LR00001], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---   \n",
    "  \n",
    "## Expiriment: Max Token Length\n",
    "\n",
    "---  \n",
    "\n",
    "By default, BERT is configured to cut sequences off at 25 tokens. While this ensures that computations don't become too large, it is also introduces an important artifact: only the first part of the reviews are considered for classification.\n",
    "\n",
    "Based on our EDA, `*****`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to change this parameter, we\n",
    "* re-initialize `bert_serving.client` with\n",
    "    * `-max_seq_len=50`\n",
    "    * `-max_seq_len=100`\n",
    "    * `-max_seq_len=200`\n",
    "* re-run with `learning_rate = 0.01` (based on previous experiment) in order to compare directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increase to 50 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# !bert-serving-start -model_dir ./uncased_L-12_H-768_A-12 -num_worker=4 -max_seq_len=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/bert_serving/client/__init__.py:286: UserWarning: some of your sentences have more tokens than \"max_seq_len=50\" set on the server, as consequence you may get less-accurate or truncated embeddings.\n",
      "here is what you can do:\n",
      "- disable the length-check by create a new \"BertClient(check_length=False)\" when you do not want to display this warning\n",
      "- or, start a new server with a larger \"max_seq_len\"\n",
      "  '- or, start a new server with a larger \"max_seq_len\"' % self.length_limit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 9\n",
      "Chunk 10\n",
      "Chunk 11\n",
      "Chunk 12\n",
      "Chunk 13\n",
      "Chunk 14\n",
      "Chunk 15\n",
      "Wrote data_000.p\n",
      "Chunk 16\n",
      "Chunk 17\n",
      "Chunk 18\n",
      "Chunk 19\n",
      "Chunk 20\n",
      "Chunk 21\n",
      "Chunk 22\n",
      "Chunk 23\n",
      "Wrote data_001.p\n",
      "Chunk 24\n",
      "Chunk 25\n",
      "Chunk 26\n",
      "Chunk 27\n",
      "Chunk 28\n",
      "Chunk 29\n",
      "Chunk 30\n",
      "Chunk 31\n",
      "Wrote data_002.p\n",
      "Chunk 32\n",
      "Chunk 33\n",
      "Chunk 34\n",
      "Chunk 35\n",
      "Chunk 36\n",
      "Chunk 37\n",
      "Chunk 38\n",
      "Chunk 39\n",
      "Wrote data_003.p\n",
      "Chunk 40\n",
      "Chunk 41\n",
      "Chunk 42\n",
      "Chunk 43\n",
      "Chunk 44\n",
      "Chunk 45\n",
      "Chunk 46\n",
      "Chunk 47\n",
      "Wrote data_004.p\n",
      "Chunk 48\n",
      "Chunk 49\n",
      "Chunk 50\n",
      "Chunk 51\n",
      "Chunk 52\n",
      "Chunk 53\n",
      "Chunk 54\n",
      "Chunk 55\n",
      "Wrote data_005.p\n",
      "Chunk 56\n",
      "Chunk 57\n",
      "Chunk 58\n",
      "Chunk 59\n",
      "Chunk 60\n",
      "Chunk 61\n",
      "Chunk 62\n",
      "Chunk 63\n",
      "Wrote data_006.p\n",
      "Chunk 64\n",
      "Chunk 65\n",
      "Chunk 66\n",
      "Chunk 67\n",
      "Chunk 68\n",
      "Chunk 69\n",
      "Chunk 70\n",
      "Chunk 71\n",
      "Wrote data_007.p\n",
      "Chunk 72\n",
      "Chunk 73\n",
      "Chunk 74\n",
      "Chunk 75\n",
      "Chunk 76\n",
      "Chunk 77\n",
      "Chunk 78\n",
      "Chunk 79\n",
      "Wrote data_008.p\n",
      "Chunk 80\n",
      "Chunk 81\n",
      "Chunk 82\n",
      "Chunk 83\n",
      "Chunk 84\n",
      "Chunk 85\n",
      "Chunk 86\n",
      "Chunk 87\n",
      "Wrote data_009.p\n",
      "Chunk 88\n",
      "Chunk 89\n",
      "Chunk 90\n",
      "Chunk 91\n",
      "Chunk 92\n",
      "Chunk 93\n",
      "Chunk 94\n",
      "Chunk 95\n",
      "Wrote data_010.p\n",
      "Chunk 96\n",
      "Chunk 97\n",
      "Chunk 98\n",
      "Chunk 99\n",
      "Chunk 100\n",
      "Chunk 101\n",
      "Chunk 102\n",
      "Chunk 103\n",
      "Wrote data_011.p\n",
      "Chunk 104\n",
      "Chunk 105\n",
      "Wrote data_012.p\n",
      "Chunk 106\n",
      "Chunk 107\n",
      "Chunk 108\n",
      "Chunk 109\n",
      "Chunk 110\n",
      "Chunk 111\n",
      "Chunk 112\n",
      "Chunk 113\n",
      "Wrote data_000.p\n",
      "Chunk 114\n",
      "Chunk 115\n",
      "Chunk 116\n",
      "Chunk 117\n",
      "Chunk 118\n",
      "Chunk 119\n",
      "Chunk 120\n",
      "Chunk 121\n",
      "Wrote data_001.p\n",
      "Chunk 122\n",
      "Chunk 123\n",
      "Chunk 124\n",
      "Chunk 125\n",
      "Chunk 126\n",
      "Chunk 127\n",
      "Chunk 128\n",
      "Chunk 129\n",
      "Wrote data_002.p\n",
      "Chunk 130\n",
      "Chunk 131\n",
      "Chunk 132\n",
      "Chunk 133\n",
      "Chunk 134\n",
      "Chunk 135\n",
      "Chunk 136\n",
      "Chunk 137\n",
      "Wrote data_003.p\n",
      "Chunk 138\n",
      "Chunk 139\n",
      "Chunk 140\n",
      "Chunk 141\n",
      "Chunk 142\n",
      "Chunk 143\n",
      "Chunk 144\n",
      "Chunk 145\n",
      "Wrote data_004.p\n",
      "Chunk 146\n",
      "Chunk 147\n",
      "Chunk 148\n",
      "Chunk 149\n",
      "Chunk 150\n",
      "Chunk 151\n",
      "Chunk 152\n",
      "Chunk 153\n",
      "Wrote data_005.p\n",
      "Chunk 154\n",
      "Chunk 155\n",
      "Chunk 156\n",
      "Chunk 157\n",
      "Chunk 158\n",
      "Chunk 159\n",
      "Chunk 160\n",
      "Chunk 161\n",
      "Wrote data_006.p\n",
      "Chunk 162\n",
      "Chunk 163\n",
      "Chunk 164\n",
      "Chunk 165\n",
      "Chunk 166\n",
      "Chunk 167\n",
      "Chunk 168\n",
      "Chunk 169\n",
      "Wrote data_007.p\n",
      "Chunk 170\n",
      "Chunk 171\n",
      "Chunk 172\n",
      "Chunk 173\n",
      "Chunk 174\n",
      "Chunk 175\n",
      "Chunk 176\n",
      "Chunk 177\n",
      "Wrote data_008.p\n",
      "Chunk 178\n",
      "Chunk 179\n",
      "Chunk 180\n",
      "Chunk 181\n",
      "Chunk 182\n",
      "Chunk 183\n",
      "Chunk 184\n",
      "Chunk 185\n",
      "Wrote data_009.p\n",
      "Chunk 186\n",
      "Chunk 187\n",
      "Chunk 188\n",
      "Chunk 189\n",
      "Chunk 190\n",
      "Chunk 191\n",
      "Chunk 192\n",
      "Chunk 193\n",
      "Wrote data_010.p\n",
      "Chunk 194\n",
      "Chunk 195\n",
      "Chunk 196\n",
      "Chunk 197\n",
      "Chunk 198\n",
      "Chunk 199\n",
      "Chunk 200\n",
      "Chunk 201\n",
      "Wrote data_011.p\n",
      "Chunk 202\n",
      "Chunk 203\n",
      "Wrote data_012.p\n",
      "CPU times: user 3.46 s, sys: 2.92 s, total: 6.37 s\n",
      "Wall time: 1h 6min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# set directories\n",
    "my_dir_train = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/aclImdb/train'\n",
    "my_dir_test  = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/aclImdb/test'\n",
    "my_dir_train_output = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/uncased_L-12_H-768_A-12/cache/train_tokens50'\n",
    "my_dir_test_output  = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/uncased_L-12_H-768_A-12/cache/test_tokens50'\n",
    "\n",
    "# build cache\n",
    "input_fn_train = cache_data(my_dir_train, my_dir_train_output, 0, 14)\n",
    "input_fn_eval = cache_data(my_dir_test, my_dir_test_output, 0, 14)\n",
    "\n",
    "# Preparation for testing\n",
    "# BERT-small, uncased [more tokens]\n",
    "dir_BSU_train_tokens50 = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/uncased_L-12_H-768_A-12/cache/train_tokens50'\n",
    "dir_BSU_test_tokens50  = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/uncased_L-12_H-768_A-12/cache/test_tokens50'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "saving to:\n",
      "\n",
      " /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_tokens50_DNN_inputNone_maxsteps2000_hu10_lr0.01_dropout0.2 \n",
      "\n",
      "\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_tokens50_DNN_inputNone_maxsteps2000_hu10_lr0.01_dropout0.2', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 100, '_save_checkpoints_secs': None, '_session_config': , '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x131af5f60>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "20000 train data points\n",
      "5000 dev data points\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/estimator/inputs/queues/feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/estimator/inputs/queues/feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py:804: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_tokens50_DNN_inputNone_maxsteps2000_hu10_lr0.01_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:loss = 97.85707, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 100 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_tokens50_DNN_inputNone_maxsteps2000_hu10_lr0.01_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 307.103\n",
      "INFO:tensorflow:loss = 78.965, step = 101 (0.327 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 200 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_tokens50_DNN_inputNone_maxsteps2000_hu10_lr0.01_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 418.293\n",
      "INFO:tensorflow:loss = 84.38834, step = 201 (0.238 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 300 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_tokens50_DNN_inputNone_maxsteps2000_hu10_lr0.01_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 472.195\n",
      "INFO:tensorflow:loss = 63.669815, step = 301 (0.212 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 400 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_tokens50_DNN_inputNone_maxsteps2000_hu10_lr0.01_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 480.056\n",
      "INFO:tensorflow:loss = 73.70733, step = 401 (0.208 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 500 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_tokens50_DNN_inputNone_maxsteps2000_hu10_lr0.01_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 510.908\n",
      "INFO:tensorflow:loss = 57.277794, step = 501 (0.196 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 600 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_tokens50_DNN_inputNone_maxsteps2000_hu10_lr0.01_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 476.965\n",
      "INFO:tensorflow:loss = 70.93406, step = 601 (0.210 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 700 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_tokens50_DNN_inputNone_maxsteps2000_hu10_lr0.01_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 385.94\n",
      "INFO:tensorflow:loss = 69.5866, step = 701 (0.259 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 800 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_tokens50_DNN_inputNone_maxsteps2000_hu10_lr0.01_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 332.355\n",
      "INFO:tensorflow:loss = 70.11516, step = 801 (0.301 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 900 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_tokens50_DNN_inputNone_maxsteps2000_hu10_lr0.01_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 326.981\n",
      "INFO:tensorflow:loss = 67.516205, step = 901 (0.307 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_tokens50_DNN_inputNone_maxsteps2000_hu10_lr0.01_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 381.311\n",
      "INFO:tensorflow:loss = 60.14179, step = 1001 (0.261 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1100 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_tokens50_DNN_inputNone_maxsteps2000_hu10_lr0.01_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 453.676\n",
      "INFO:tensorflow:loss = 60.039207, step = 1101 (0.221 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1200 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_tokens50_DNN_inputNone_maxsteps2000_hu10_lr0.01_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 492.095\n",
      "INFO:tensorflow:loss = 67.40864, step = 1201 (0.203 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1300 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_tokens50_DNN_inputNone_maxsteps2000_hu10_lr0.01_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 445.873\n",
      "INFO:tensorflow:loss = 69.00589, step = 1301 (0.225 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1400 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_tokens50_DNN_inputNone_maxsteps2000_hu10_lr0.01_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 374.935\n",
      "INFO:tensorflow:loss = 55.704803, step = 1401 (0.267 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1500 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_tokens50_DNN_inputNone_maxsteps2000_hu10_lr0.01_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 372.775\n",
      "INFO:tensorflow:loss = 65.06263, step = 1501 (0.268 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1563 into /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_tokens50_DNN_inputNone_maxsteps2000_hu10_lr0.01_dropout0.2/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 32.07218.\n",
      "CPU times: user 7.56 s, sys: 1.18 s, total: 8.74 s\n",
      "Wall time: 5.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "hidden_units = [10]\n",
    "learning_rate = 0.01\n",
    "bert_model = 'bert_uncased_small_tokens50'\n",
    "classifier = 'DNN'\n",
    "dropout_rate = 0.2\n",
    "train_max_steps = 2000\n",
    "input_size = None # all\n",
    "\n",
    "dir_models = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_{}_{}_input{}_maxsteps{}_hu{}_lr{}_dropout{}'.format(\n",
    "    bert_model, classifier, input_size, train_max_steps, '_'.join([str(x) for x in hidden_units]), learning_rate, dropout_rate)\n",
    "print ('\\n\\nsaving to:\\n\\n',dir_models, '\\n\\n')\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "run_config = RunConfig(model_dir=dir_models, session_config=config,save_checkpoints_steps=100)\n",
    "estimator = DNNClassifier(\n",
    "    hidden_units=hidden_units,\n",
    "    feature_columns=[tf.feature_column.numeric_column('feature', shape=(768,))],\n",
    "    n_classes=2,\n",
    "    config=run_config,\n",
    "    optimizer=tf.train.AdagradOptimizer(learning_rate=learning_rate),\n",
    "    dropout=dropout_rate)\n",
    "train_input_fn, dev_input_fn = get_input_fn(dir_BSU_train_tokens50, input_size) # BERT-small, uncased [tokens50]\n",
    "estimator.train(input_fn=train_input_fn, max_steps=train_max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-07-09:24:54\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_tokens50_DNN_inputNone_maxsteps2000_hu10_lr0.01_dropout0.2/model.ckpt-1563\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-07-09:24:55\n",
      "INFO:tensorflow:Saving dict for global step 1563: accuracy = 0.7546, accuracy_baseline = 0.5, auc = 0.83904177, auc_precision_recall = 0.84167016, average_loss = 0.49245283, global_step = 1563, label/mean = 0.5, loss = 61.556602, precision = 0.7524792, prediction/mean = 0.4926438, recall = 0.7588\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1563: /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_tokens50_DNN_inputNone_maxsteps2000_hu10_lr0.01_dropout0.2/model.ckpt-1563\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TK_50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.754600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_baseline</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc</th>\n",
       "      <td>0.839042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc_precision_recall</th>\n",
       "      <td>0.841670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>average_loss</th>\n",
       "      <td>0.492453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label/mean</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loss</th>\n",
       "      <td>61.556602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.752479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction/mean</th>\n",
       "      <td>0.492644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.758800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>global_step</th>\n",
       "      <td>1563.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            TK_50\n",
       "accuracy                 0.754600\n",
       "accuracy_baseline        0.500000\n",
       "auc                      0.839042\n",
       "auc_precision_recall     0.841670\n",
       "average_loss             0.492453\n",
       "label/mean               0.500000\n",
       "loss                    61.556602\n",
       "precision                0.752479\n",
       "prediction/mean          0.492644\n",
       "recall                   0.758800\n",
       "global_step           1563.000000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result\n",
    "dfTK_50 = pd.DataFrame.from_dict(estimator.evaluate(dev_input_fn), orient='index', columns=['TK_50'])\n",
    "dfTK_50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increase to 100 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# !bert-serving-start -model_dir ./uncased_L-12_H-768_A-12 -num_worker=4 -max_seq_len=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/bert_serving/client/__init__.py:286: UserWarning: some of your sentences have more tokens than \"max_seq_len=50\" set on the server, as consequence you may get less-accurate or truncated embeddings.\n",
      "here is what you can do:\n",
      "- disable the length-check by create a new \"BertClient(check_length=False)\" when you do not want to display this warning\n",
      "- or, start a new server with a larger \"max_seq_len\"\n",
      "  '- or, start a new server with a larger \"max_seq_len\"' % self.length_limit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote data_000.p\n",
      "Wrote data_001.p\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-9a6228d13848>\u001b[0m in \u001b[0;36mcache_data\u001b[0;34m(data_dir, dest_dir, start_chunk, end_chunk)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchunk_num\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mend_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_encodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Wrote data_{:03d}.p'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdest_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data_{:03d}.p'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-0890dc507e63>\u001b[0m in \u001b[0;36mget_encodes\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_encodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# x is `batch_size` of lines, each of which is a json object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m# randomly choose a label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/bert_serving/client/__init__.py\u001b[0m in \u001b[0;36marg_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceiver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetsockopt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRCVTIMEO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_e\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                 t_e = TimeoutError(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/bert_serving/client/__init__.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, texts, blocking, is_tokenized, show_tokens)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblocking\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_info_available\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mshow_tokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/bert_serving/client/__init__.py\u001b[0m in \u001b[0;36m_recv_ndarray\u001b[0;34m(self, wait_for_req_id)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait_for_req_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait_for_req_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0marr_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjsonapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dtype'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/bert_serving/client/__init__.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, wait_for_req_id)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0;31m# receive a response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceiver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m                 \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0many\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreasons\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mSocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mmight\u001b[0m \u001b[0mfail\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \"\"\"\n\u001b[0;32m--> 470\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsockopt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRCVMORE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# set directories\n",
    "my_dir_train = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/aclImdb/train'\n",
    "my_dir_test  = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/aclImdb/test'\n",
    "my_dir_train_output = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/uncased_L-12_H-768_A-12/cache/train_tokens100'\n",
    "my_dir_test_output  = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/uncased_L-12_H-768_A-12/cache/test_tokens100'\n",
    "\n",
    "# build cache\n",
    "input_fn_train = cache_data(my_dir_train, my_dir_train_output, 0, 14)\n",
    "input_fn_eval = cache_data(my_dir_test, my_dir_test_output, 0, 14)\n",
    "\n",
    "# Preparation for testing\n",
    "# BERT-small, uncased [more tokens]\n",
    "dir_BSU_train_tokens100 = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/uncased_L-12_H-768_A-12/cache/train_tokens100'\n",
    "dir_BSU_test_tokens100  = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/uncased_L-12_H-768_A-12/cache/test_tokens100'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "hidden_units = [10]\n",
    "learning_rate = 0.01\n",
    "bert_model = 'bert_uncased_small_tokens100'\n",
    "classifier = 'DNN'\n",
    "dropout_rate = 0.2\n",
    "train_max_steps = 2000\n",
    "input_size = None # all\n",
    "\n",
    "dir_models = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_{}_{}_input{}_maxsteps{}_hu{}_lr{}_dropout{}'.format(\n",
    "    bert_model, classifier, input_size, train_max_steps, '_'.join([str(x) for x in hidden_units]), learning_rate, dropout_rate)\n",
    "print ('\\n\\nsaving to:\\n\\n',dir_models, '\\n\\n')\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "run_config = RunConfig(model_dir=dir_models, session_config=config,save_checkpoints_steps=100)\n",
    "estimator = DNNClassifier(\n",
    "    hidden_units=hidden_units,\n",
    "    feature_columns=[tf.feature_column.numeric_column('feature', shape=(768,))],\n",
    "    n_classes=2,\n",
    "    config=run_config,\n",
    "    optimizer=tf.train.AdagradOptimizer(learning_rate=learning_rate),\n",
    "    dropout=dropout_rate)\n",
    "train_input_fn, dev_input_fn = get_input_fn(dir_BSU_train_tokens100, input_size) # BERT-small, uncased [tokens50]\n",
    "estimator.train(input_fn=train_input_fn, max_steps=train_max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-07-09:24:40\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_tokens50_DNN_inputNone_maxsteps2000_hu10_lr0.01_dropout0.2/model.ckpt-1563\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-07-09:24:41\n",
      "INFO:tensorflow:Saving dict for global step 1563: accuracy = 0.7546, accuracy_baseline = 0.5, auc = 0.83904177, auc_precision_recall = 0.84167016, average_loss = 0.49245283, global_step = 1563, label/mean = 0.5, loss = 61.556602, precision = 0.7524792, prediction/mean = 0.4926438, recall = 0.7588\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1563: /Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_bert_uncased_small_tokens50_DNN_inputNone_maxsteps2000_hu10_lr0.01_dropout0.2/model.ckpt-1563\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TK_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.754600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_baseline</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc</th>\n",
       "      <td>0.839042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc_precision_recall</th>\n",
       "      <td>0.841670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>average_loss</th>\n",
       "      <td>0.492453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label/mean</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loss</th>\n",
       "      <td>61.556602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.752479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction/mean</th>\n",
       "      <td>0.492644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.758800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>global_step</th>\n",
       "      <td>1563.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           TK_100\n",
       "accuracy                 0.754600\n",
       "accuracy_baseline        0.500000\n",
       "auc                      0.839042\n",
       "auc_precision_recall     0.841670\n",
       "average_loss             0.492453\n",
       "label/mean               0.500000\n",
       "loss                    61.556602\n",
       "precision                0.752479\n",
       "prediction/mean          0.492644\n",
       "recall                   0.758800\n",
       "global_step           1563.000000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result\n",
    "dfTK_100 = pd.DataFrame.from_dict(estimator.evaluate(dev_input_fn), orient='index', columns=['TK_100'])\n",
    "dfTK_100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increase to 200 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# set directories\n",
    "my_dir_train = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/aclImdb/train'\n",
    "my_dir_test  = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/aclImdb/test'\n",
    "my_dir_train_output = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/uncased_L-12_H-768_A-12/cache/train_tokens200'\n",
    "my_dir_test_output  = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/uncased_L-12_H-768_A-12/cache/test_tokens200'\n",
    "\n",
    "# build cache\n",
    "input_fn_train = cache_data(my_dir_train, my_dir_train_output, 0, 14)\n",
    "input_fn_eval = cache_data(my_dir_test, my_dir_test_output, 0, 14)\n",
    "\n",
    "# Preparation for testing\n",
    "# BERT-small, uncased [more tokens]\n",
    "dir_BSU_train_tokens200 = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/uncased_L-12_H-768_A-12/cache/train_tokens200'\n",
    "dir_BSU_test_tokens200  = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/uncased_L-12_H-768_A-12/cache/test_tokens200'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "hidden_units = [10]\n",
    "learning_rate = 0.01\n",
    "bert_model = 'bert_uncased_small_tokens200'\n",
    "classifier = 'DNN'\n",
    "dropout_rate = 0.2\n",
    "train_max_steps = 2000\n",
    "input_size = None # all\n",
    "\n",
    "dir_models = '/Users/jlc/Google Drive/_code/MIDS_W266/BERT_Imdb/trained_models/imdb_{}_{}_input{}_maxsteps{}_hu{}_lr{}_dropout{}'.format(\n",
    "    bert_model, classifier, input_size, train_max_steps, '_'.join([str(x) for x in hidden_units]), learning_rate, dropout_rate)\n",
    "print ('\\n\\nsaving to:\\n\\n',dir_models, '\\n\\n')\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "run_config = RunConfig(model_dir=dir_models, session_config=config,save_checkpoints_steps=100)\n",
    "estimator = DNNClassifier(\n",
    "    hidden_units=hidden_units,\n",
    "    feature_columns=[tf.feature_column.numeric_column('feature', shape=(768,))],\n",
    "    n_classes=2,\n",
    "    config=run_config,\n",
    "    optimizer=tf.train.AdagradOptimizer(learning_rate=learning_rate),\n",
    "    dropout=dropout_rate)\n",
    "train_input_fn, dev_input_fn = get_input_fn(dir_BSU_train_tokens200, input_size) # BERT-small, uncased [tokens50]\n",
    "estimator.train(input_fn=train_input_fn, max_steps=train_max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result\n",
    "dfTK_200 = pd.DataFrame.from_dict(estimator.evaluate(dev_input_fn), orient='index', columns=['TK_200'])\n",
    "dfTK_200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vpd.concat([df_LR01, dfTK_50, dfTK_100, dfTK_200], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---   \n",
    "  \n",
    "# Expiriment:\n",
    "\n",
    "---  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---   \n",
    "  \n",
    "# Expiriment:\n",
    "\n",
    "---  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---   \n",
    "  \n",
    "# Expiriment:\n",
    "\n",
    "---  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---   \n",
    "  \n",
    "# Expiriment:\n",
    "\n",
    "---  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---   \n",
    "  \n",
    "# Expiriment:\n",
    "\n",
    "---  "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "350px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "321px",
    "left": "23px",
    "right": "20px",
    "top": "601px",
    "width": "318px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
